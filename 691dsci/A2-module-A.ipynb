{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vT829vWKHgoL"
   },
   "source": [
    "## Module submission header\n",
    "### Submission preparation instructions \n",
    "_Completion of this header is mandatory, subject to a 2-point deduction to the assignment._ Only add plain text in the designated areas, i.e., replacing the relevant 'NA's. You must fill out all group member Names and Drexel email addresses in the below markdown list, under header __Module submission group__. It is required to fill out descriptive notes pertaining to any tutoring support received in the completion of this submission under the __Additional submission comments__ section at the bottom of the header. If no tutoring support was received, leave NA in place. You may as well list other optional comments pertaining to the submission at bottom. _Any distruption of this header's formatting will make your group liable to the 2-point deduction._\n",
    "\n",
    "### Module submission group\n",
    "- Group member 1\n",
    "    - Name: Xi Chen\n",
    "    - Email: xc98@drexel.edu\n",
    "- Group member 2\n",
    "    - Name: Tai Nguyen\n",
    "    - Email: tdn47@drexel.edu\n",
    "- Group member 3\n",
    "    - Name: Tien Nguyen\n",
    "    - Email: thn44@drexel.edu\n",
    "- Group member 4\n",
    "    - Name: Raymond Yung\n",
    "    - Email: raymond.yung@drexel.edu\n",
    "\n",
    "### Additional submission comments\n",
    "- Tutoring support received: NA\n",
    "- Other (other): NA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3HEYAf_bjRJ6"
   },
   "source": [
    "# DSCI 691: Natural language processing with deep learning <br> Assignment 2: Abstracting Summaries of the News\n",
    "## Data and Utilities \n",
    "Here, we'll be working again with the same linked NewsTweet data and some essential utilities presented in the __Chapter 1 Notes__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "RfpwpFTS1ncL"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "newstweet = json.load(open('./data/newstweet-subsample-linked.json'))\n",
    "# exec(open('./01-utilities.py').read())\n",
    "from utilities import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pUcZOyTMHgoO"
   },
   "source": [
    "## Overview \n",
    "The purpose of this assignment (37 pts) is to gain some experience with the extremely important NLP task called language modeling (LM'ing). We'll explore a traditional $n$-gram approach to help elucidate the central challenges with the problem.\n",
    "\n",
    "Since this is an LM'ing assignment, for the sanity checks we'll be working on a single document from the data set throughout, focused on a Robert Downey Jr. movie. But in principle you should be able to apply this assignment to any of the articles and generate text for summaries, and you should&mdash;it's fun!\n",
    "\n",
    "Here's the article of focus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "ZHXclFPiHgoP",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robert Downey Jr. would rather talk to animals than people as the new Dr. Dolittle. But, as seen this official “Dolittle” trailer released Sunday, he is soon forced to leave his hideaway and set sail across the sea to a mythical island.\n",
      "\n",
      "“We have no choice but to embark on this perilous journey,” Downey whispers to his feathered and furry friends.\n",
      "\n",
      "And that involves getting into all sorts of dangerous situations, like being chained in a medieval dungeon with a tiger who greets him with, “hello, lunch” and then jumps him. But not to worry — a gorilla voiced by Rami Malek comes to his rescue.\n",
      "\n",
      "Also Read: Universal Moves Robert Downey Jr.'s 'Voyage of Doctor Dolittle' to January 2020\n",
      "\n",
      "“Dolittle” tells the story of famed doctor and veterinarian during the time of Queen Victoria’s England, Dr. John Dolittle, who returns to action after the loss of his wife seven years earlier caused him to become a hermit who only talks to animals. But when the young queen (Jessie Buckley, “Wild Rose”) falls ill, he goes on an adventure to find her a cure. He is joined on this quest by a young apprentice (Harry Collett, “Dunkirk”) and several animal friends, including an anxious gorilla (Malek), an enthusiastic duck (Octavia Spencer), an cynical ostrich (Kumail Nanjiani), an upbeat polar bear (John Cena), and a headstrong parrot (Emma Thompson).\n",
      "\n",
      "Antonio Banderas also stars as Rassouli, along with Michael Sheen as Mudfly. Additional voice performers include Marion Cotillard, Frances de la Tour, Carmen Ejogo, Ralph Fiennes, Selena Gomez, Tom Holland, and Craig Robinson.\n",
      "\n",
      "“Dolittle” is directed by Stephen Gaghan (“Syriana,” “Traffic”), and produced by Joe Roth and Jeff Kirschenbaum under their Roth/Kirschenbaum Films banner (“Alice in Wonderland,” “Maleficent”), as well as and Susan Downey (“Sherlock Holmes” franchise, “The Judge”) for Team Downey. Downey Jr. executive produces along iwth Sarah Bradshaw (“The Mummy,” “Maleficent”) and Zachary Roth (“Maleficent: Mistress of Evil”).\n"
     ]
    }
   ],
   "source": [
    "print(newstweet[5]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ms4rr6CnHgoQ"
   },
   "source": [
    "As we continue, we'll explore ways that we can use sparse models to regenerate this document, and along the way we'll get some experience with model-sampling and perplexity as a performance measure.\n",
    "\n",
    "## Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gPij53l3HgoS"
   },
   "source": [
    "### 1. (3 pts) Build an $n$-gram counter\n",
    "Given an input list of `tokens`, use list slices to complete the `count(tokens, n = 1)` function to produce and return the `ngram_counts` object, as a `Counter()` of `n`-sized tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "VCzJ7_8eHgoT"
   },
   "outputs": [],
   "source": [
    "# A1:Function(3/3)\n",
    "\n",
    "def count(tokens, n = 1):\n",
    "    \n",
    "    #--- your code starts here\n",
    "    ngram_counts = Counter([tuple(tokens[i: i+n]) for i in range(len(tokens)-n +1)])\n",
    "    \n",
    "    #--- your code stops here\n",
    "    \n",
    "    return ngram_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "nImvbllfHgoT"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({('this', ' ', 'is', ' ', 'an'): 1,\n",
       "         (' ', 'is', ' ', 'an', ' '): 1,\n",
       "         ('is', ' ', 'an', ' ', 'example'): 1,\n",
       "         (' ', 'an', ' ', 'example', ' '): 1,\n",
       "         ('an', ' ', 'example', ' ', 'of'): 1,\n",
       "         (' ', 'example', ' ', 'of', ' '): 1,\n",
       "         ('example', ' ', 'of', ' ', 'a'): 1,\n",
       "         (' ', 'of', ' ', 'a', ' '): 1,\n",
       "         ('of', ' ', 'a', ' ', 'token'): 1,\n",
       "         (' ', 'a', ' ', 'token', ' '): 1,\n",
       "         ('a', ' ', 'token', ' ', 'stream'): 1})"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A1:SanityCheck\n",
    "\n",
    "count([\"this\", \" \", \"is\", \" \", \"an\", \" \", \"example\", \" \", \n",
    "       \"of\", \" \", \"a\", \" \", \"token\", \" \", \"stream\"], 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vvcnCShfHgoT"
   },
   "source": [
    "For reference, your output should be:\n",
    "\n",
    "```\n",
    "Counter({(' ', 'a', ' ', 'token', ' '): 1,\n",
    "         (' ', 'an', ' ', 'example', ' '): 1,\n",
    "         (' ', 'example', ' ', 'of', ' '): 1,\n",
    "         (' ', 'is', ' ', 'an', ' '): 1,\n",
    "         (' ', 'of', ' ', 'a', ' '): 1,\n",
    "         ('a', ' ', 'token', ' ', 'stream'): 1,\n",
    "         ('an', ' ', 'example', ' ', 'of'): 1,\n",
    "         ('example', ' ', 'of', ' ', 'a'): 1,\n",
    "         ('is', ' ', 'an', ' ', 'example'): 1,\n",
    "         ('of', ' ', 'a', ' ', 'token'): 1,\n",
    "         ('this', ' ', 'is', ' ', 'an'): 1})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HODpwQywHgoT"
   },
   "source": [
    "## 2. (4 pts) Build $n$-gram frequencies up to a size\n",
    "Here, your job will be to apply the `count` function to build $n$-gram frequency distributions up to a given maximum size. To do this, complete the `make_ngram_frequency(documents, n = 1, space = True)`\n",
    "The function's main argument is `documents`, which will be a list of strings, and the function's only output will be `ngram_frequencies`, which will be a list of $n$-gram `Counter()`s, up to a specified (by `n`) size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "ww96cWocHgoU"
   },
   "outputs": [],
   "source": [
    "# A2:Function(4/4)\n",
    "\n",
    "def make_ngram_frequency(documents, n = 1, space = True):\n",
    "    ngram_frequencies = []\n",
    "    \n",
    "    #--- your code starts here\n",
    "    for i in range(1, n+1):\n",
    "        counter_n = Counter()\n",
    "        for doc in documents:\n",
    "            counter_n.update(count(tokenize(doc, space=space), n=i))\n",
    "        ngram_frequencies.append(counter_n)\n",
    "    #--- your code stops here\n",
    "    return ngram_frequencies, {t[0]: i for i, t in enumerate(ngram_frequencies[0].keys())}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "GG9PP5sCHgoU"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A2:SanityCheck\n",
    "\n",
    "n = 9\n",
    "ngram_frequencies, type_index = make_ngram_frequency([x['text'].lower() for x in newstweet], n = n)\n",
    "ngram_frequencies[5][tuple(tokenize('robert downey jr.'))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pok_h1CwHgoU"
   },
   "source": [
    "For reference, your output should be:\n",
    "```\n",
    "8\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tRL3wuDGHgoW"
   },
   "source": [
    "## 3. (6 pts) Build the standard LM\n",
    "Now it's time for the model, i.e., computing the LM probabilities from __Section 2.1.4.1__. This will entail completing the two tricks, the first of which, $\\varepsilon$-smoothing we re-define (for stability) for some small $\\varepsilon>0$ as:\n",
    "$$\n",
    "\\hat{P}(t_n|t_1, t_2, \\cdots t_{n-1}) = \\frac{\\frac{\\varepsilon}{|W|} + f(t_1, t_2,\\cdots, t_n)}{\\varepsilon + f(t_1, t_2,\\cdots, t_{n-1})}\n",
    "$$\n",
    "where some small, constant non-zero weight ($\\varepsilon$) is distributed to each type of the model in _every_ context, regardless of it's actual appearance in the data.\n",
    "\n",
    "The second component you'll have to sort out is _backoff_, where the desired probabilities are approximated via the next-lower-$n$ context adjacent to the prediction point:\n",
    "$$\n",
    "\\hat{P}(t_n|t_1, \\cdots t_{n-1})\\approx\\hat{P}(t_n|t_2, \\cdots t_{n-1}).\n",
    "$$\n",
    "\n",
    "For both cases, this amounts to determining `t_Ps`, as a `Counter()` of types-as-keys with probability values and thus completing the function:\n",
    "```\n",
    "P_next(gram, ngram_frequencies, type_index, epsilon = 0.1)\n",
    "```\n",
    "for which `gram` corresponds to the vector, $\\vec{t} = [t_1,\\cdots,t_{n-1}]$ of tokens preceeding the prediction point.  \n",
    "\n",
    "In this, $\\varepsilon_w$  (coded as `epsw`) will indicate _the portion of the total mass of the smoothing parameter, $\\varepsilon$, assigned to $w$_. Hence, the default setting `epsilon = 0.1` will mean:\n",
    "$$\n",
    "\\varepsilon_w = \\frac{0.1}{|W|}.\n",
    "$$\n",
    "which should allow for convienient parameterization of _slight_ smoothings, which won't totally swamp the model's performance.\n",
    "\n",
    "[Hint. use the `n1` (context size) to navigate the `ngram_frequencies` object, and don't be afraid to slice `gram`s]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "B20owdEAHgoW"
   },
   "outputs": [],
   "source": [
    "# A3:Function(6/6)\n",
    "\n",
    "def P_next(gram, ngram_frequencies, type_index, epsilon = .1):\n",
    "    n1 = len(gram)\n",
    "    epsw = epsilon/len(type_index)\n",
    "    if gram in ngram_frequencies[n1-1]: ## use gram to condition frequencies with epsilon-smoothing\n",
    "        \n",
    "        #--- your code starts here\n",
    "        prob_gram = ngram_frequencies[n1-1][gram] + epsilon\n",
    "        t_Ps = {}\n",
    "        for tok in type_index:\n",
    "            next_gram = tuple(list(gram) + [tok])\n",
    "            if next_gram in ngram_frequencies[n1]:\n",
    "                prob_next_tok = ngram_frequencies[n1][next_gram] + epsw\n",
    "            else:\n",
    "                prob_next_tok = epsw\n",
    "            t_Ps[tok] = prob_next_tok / prob_gram\n",
    "            \n",
    "        t_Ps = Counter(t_Ps)\n",
    "        #--- your code stops here\n",
    "        \n",
    "    else: ## recursively back off to lower-n model\n",
    "        \n",
    "        #--- your code starts here\n",
    "        t_Ps = P_next(gram[1:], ngram_frequencies, type_index, epsilon)\n",
    "        #--- your code stops here\n",
    "\n",
    "    \n",
    "    return t_Ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "daNkyhACHgoW"
   },
   "source": [
    "For reference, your output should be:\n",
    "```\n",
    "[1.0,\n",
    " [('adventure', 0.9090937517766786),\n",
    "  ('aew', 2.8426857695150374e-06),\n",
    "  (' ', 2.8426857695150374e-06),\n",
    "  ('star', 2.8426857695150374e-06),\n",
    "  ('matt', 2.8426857695150374e-06),\n",
    "  ('jackson', 2.8426857695150374e-06),\n",
    "  ('of', 2.8426857695150374e-06),\n",
    "  ('the', 2.8426857695150374e-06),\n",
    "  ('young', 2.8426857695150374e-06),\n",
    "  ('bucks', 2.8426857695150374e-06)],\n",
    " [(' ', 0.9090937517766786),\n",
    "  ('aew', 2.8426857695150374e-06),\n",
    "  ('star', 2.8426857695150374e-06),\n",
    "  ('matt', 2.8426857695150374e-06),\n",
    "  ('jackson', 2.8426857695150374e-06),\n",
    "  ('of', 2.8426857695150374e-06),\n",
    "  ('the', 2.8426857695150374e-06),\n",
    "  ('young', 2.8426857695150374e-06),\n",
    "  ('bucks', 2.8426857695150374e-06),\n",
    "  ('is', 2.8426857695150374e-06)]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "7PwtXBzdHgoW"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0,\n",
       " [('adventure', 0.9090937517766786),\n",
       "  ('aew', 2.8426857695150374e-06),\n",
       "  (' ', 2.8426857695150374e-06),\n",
       "  ('star', 2.8426857695150374e-06),\n",
       "  ('matt', 2.8426857695150374e-06),\n",
       "  ('jackson', 2.8426857695150374e-06),\n",
       "  ('of', 2.8426857695150374e-06),\n",
       "  ('the', 2.8426857695150374e-06),\n",
       "  ('young', 2.8426857695150374e-06),\n",
       "  ('bucks', 2.8426857695150374e-06)],\n",
       " [(' ', 0.9090937517766786),\n",
       "  ('aew', 2.8426857695150374e-06),\n",
       "  ('star', 2.8426857695150374e-06),\n",
       "  ('matt', 2.8426857695150374e-06),\n",
       "  ('jackson', 2.8426857695150374e-06),\n",
       "  ('of', 2.8426857695150374e-06),\n",
       "  ('the', 2.8426857695150374e-06),\n",
       "  ('young', 2.8426857695150374e-06),\n",
       "  ('bucks', 2.8426857695150374e-06),\n",
       "  ('is', 2.8426857695150374e-06)]]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A3:SanityCheck\n",
    "\n",
    "[np.nansum([x for x in P_next(tuple(tokenize(\"he goes on an \")), ngram_frequencies, type_index).values()]), \n",
    " list(P_next(tuple(tokenize(\"he goes on an \")), ngram_frequencies, type_index).most_common(10)), \n",
    " list(P_next(tuple(tokenize(\" he goes on an\")), ngram_frequencies, type_index).most_common(10))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oFDZy00eHgoX"
   },
   "source": [
    "### 4. (7 pts) Build a model sampler\n",
    "Now that we have a LM, we need a way to sample from it. To start complete the function:\n",
    "```\n",
    "sample_LM(gram, LM_args, top = 1., LM = P_next)\n",
    "``` \n",
    "which must perform a weghted random sample via `np.random.choice()`, using the `gram` for the context of a prediction point (as in __Part 4.__, for `P_next()`). However, this sampler must deploy one of two sampling algorithms, as specified by the `top` parameter. Specifically:\n",
    "1. when `type(top) == float`, the floating point value of `top` should represent the cumulative probabiliy of top-scoring predictions to weight a sample from; and\n",
    "2. when `type(top) == int`, the integer value of `top` should represent the `top` highest-ranking predicitons to weight a sample from.\n",
    "\n",
    "In case (1), the sample might range over many or few possibilities, depending on the confusion of the model at the point of the prediction, and in case (2) the sample might be constrained to a limited vocabulary at each step. However, in both your code should use, e.g., a boolean mask to filter the `Ps` (prediciton probabilities) and `ts` (prediction types) down to just those in the `top`, i.e., 'viable' set that will be passed to the sampler.\n",
    "\n",
    "Note: in both cases your filtered prediction probabilities (`Ps`) must be re-normalized for the weighted random sample!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "mJj7h6wrHgoX"
   },
   "outputs": [],
   "source": [
    "# A4:Function(7/7)\n",
    "\n",
    "def sample_LM(gram, LM_args, top = 1., LM = P_next):\n",
    "    \n",
    "    Ps = LM(gram, *LM_args)\n",
    "    ts, Ps = map(np.squeeze, zip(*Ps.most_common()))\n",
    "    Ps /= Ps.sum()\n",
    "    \n",
    "    #--- your code starts here\n",
    "    if isinstance(top, float):\n",
    "        indices = (top - Ps.cumsum()) >= 0\n",
    "        Ps = Ps[indices]\n",
    "        ts = ts[indices]\n",
    "    elif isinstance(top, int):\n",
    "        Ps = Ps[:top]\n",
    "        ts = ts[:top]\n",
    "\n",
    "    Ps /= Ps.sum()\n",
    "        \n",
    "    #--- your code stops here\n",
    "    \n",
    "    s = np.random.choice(ts, size=1, replace=False, p=Ps)[0]\n",
    "    \n",
    "    return s\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "162R533zHgoX"
   },
   "source": [
    "For reference, your output should be:\n",
    "```\n",
    "'adventure'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "mDlCV8DOHgoY"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'adventure'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A4:SanityCheck\n",
    "\n",
    "np.random.seed(691)\n",
    "sample_LM(tuple(tokenize(\"he goes on an \")), (ngram_frequencies, type_index, 0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kBAaCpTuHgoY"
   },
   "source": [
    "### 5. (5 pts) Build a recitation function for the LM\n",
    "Here, our goal will be to have the LM 'recite' a given `document` string input. In particular, your job is to complete the function:\n",
    "```\n",
    "recitation, likelihood = recite(document, LM_args, LM = P_next, n = 5, top = 1., verbose = True)\n",
    "```\n",
    "which has the following arguments:\n",
    "- `document`: a string to be modeled by its ngrams\n",
    "- `LM_args`: a tuple of all arguments to be passed to the LM\n",
    "- `LM (= P_next)`: the LM function to be operated for the recitation\n",
    "- `n (= 5)`: an integer number indicating the gram-size to model from\n",
    "- `top (= 1.)`: the sampling paramater for the `sample_LM` function\n",
    "- `verbose (= True)`: a boolean, indicating whether the model should print the text it produces, while operating\n",
    "\n",
    "and has the following return values are:\n",
    "- `recitation`: a list of the tokens which the LM _predicts_, in order\n",
    "- `likelihood`: a list of the probabilities for the _correct_ targets of the LM as it operates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "NVaPoaFVHgoY"
   },
   "outputs": [],
   "source": [
    "# A5:Function(5/5)\n",
    "\n",
    "def recite(document, LM_args, LM = P_next, n = 5, top = 1., verbose = True):\n",
    "    tokens = tokenize(document)\n",
    "    ngram_stream = [tuple(tokens[i:i+n]) for i in range(0,len(tokens) - n + 1)]\n",
    "\n",
    "    if verbose:\n",
    "        print(\"generated document, starting from \\\"\"+\"\".join(ngram_stream[0][:-1])+\"\\\":\\n\")\n",
    "        \n",
    "    likelihood = []; recitation = []\n",
    "    for ix, ngram in enumerate(ngram_stream):\n",
    "        \n",
    "        #--- your code starts here\n",
    "        target = ngram[-1]\n",
    "        n1gram = tuple(ngram[:-1])\n",
    "        Ps = LM(n1gram, *LM_args)\n",
    "        likelihood.append(Ps[target])\n",
    "        recitation.append(sample_LM(n1gram, LM_args, LM = LM, top = top))\n",
    "        \n",
    "        #--- your code stops here\n",
    "        \n",
    "        if verbose:\n",
    "            print(recitation[-1], end = '')\n",
    "    \n",
    "    return recitation, likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TIv_7qmhHgoZ"
   },
   "source": [
    "For reference, your output should be:\n",
    "```\n",
    "generated document, starting from \"robert downey \":\n",
    "\n",
    "jr. ( aikman have to patients than people as of earth batman. birx  but, for i this?official portdolittle” is released sunday, either wasn't not forced to “ nikola post and set them across the country,( a bout island.\n",
    "\n",
    "athe need to higher but to get on quests backdrop journey,” downey whispers to his hamstring and furry friends.\n",
    "\n",
    "\",this’your lots into all of of dangerous situations, like me chained in a federal dungeon with a similar who greets him with  buthello, lunch” and it they him. he even penetrating receive about a businessman voiced by none malek comes to takedown fans.\n",
    "\n",
    "also,overshadowed: delta moves robert downey jr.  'voyage of doctor credentialing to january 2020 \n",
    "floyddolittle” trailer the moderator a the doctor and whistleblower during the early these year victoria’s england, dismissed. myron dolittle, who are to service after the police,of his war dorothy years,of,caused him to be the thing who only talks to appear  but there i names,hawaiian (jessieconsiderationbuckley, “wild rose”) falls ill, he has down sale industrial to find out a would-be. remember played choosing on stage usa by a bipartisan voter (harry collett, “dunkirk”) and the individuals friends, located at anxious gorilla (malek), an organization duck (octavia spencer), an atlanta-based ostrich (kumail nanjiani), an abolitionist polar bear (john cena), and if chest-strapped parrot (emma thompson).\n",
    "\n",
    "the brown's also stars as the, along with added sheen as mudfly. additional reporting performers include marion cotillard, frances de la marihuana, 1972 ejogo, ralph fiennes, selena was  27 jurich, and other robinson.\n",
    "\n",
    "“he” is a more james hawking (“syriana,” “traffic”), as the by nadal roth and jeff kirschenbaum under their “/kirschenbaum films banner (“alice in wonderland,” “calf”), and of as the susan downey,(“sherlock holmes” franchise, aewthe power”) for $ downey. downey jr. do produces along iwth sarah bradshaw (“the mummy,” “traffic”),and quarterback roth (“maleficent: mistress of evil ringed.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "-EFAojgvHgoZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated document, starting from \"robert downey \":\n",
      "\n",
      "jr. ( aikman have to patients than people as of earth batman. birx  but, for i this?official portdolittle” is released sunday, either wasn't not forced to “ nikola post and set them across the country,( a bout island.\n",
      "\n",
      "athe need to higher but to get on quests backdrop journey,” downey whispers to his hamstring and furry friends.\n",
      "\n",
      "\",this’your lots into all of of dangerous situations, like me chained in a federal dungeon with a similar who greets him with  buthello, lunch” and it they him. he even penetrating receive about a businessman voiced by none malek comes to takedown fans.\n",
      "\n",
      "also,overshadowed: delta moves robert downey jr.  'voyage of doctor credentialing to january 2020 \n",
      "floyddolittle” trailer the moderator a the doctor and whistleblower during the early these year victoria’s england, dismissed. myron dolittle, who are to service after the police,of his war dorothy years,of,caused him to be the thing who only talks to appear  but there i names,hawaiian (jessieconsiderationbuckley, “wild rose”) falls ill, he has down sale industrial to find out a would-be. remember played choosing on stage usa by a bipartisan voter (harry collett, “dunkirk”) and the individuals friends, located at anxious gorilla (malek), an organization duck (octavia spencer), an atlanta-based ostrich (kumail nanjiani), an abolitionist polar bear (john cena), and if chest-strapped parrot (emma thompson).\n",
      "\n",
      "the brown's also stars as the, along with added sheen as mudfly. additional reporting performers include marion cotillard, frances de la marihuana, 1972 ejogo, ralph fiennes, selena was  27 jurich, and other robinson.\n",
      "\n",
      "“he” is a more james hawking (“syriana,” “traffic”), as the by nadal roth and jeff kirschenbaum under their “/kirschenbaum films banner (“alice in wonderland,” “calf”), and of as the susan downey,(“sherlock holmes” franchise, aewthe power”) for $ downey. downey jr. do produces along iwth sarah bradshaw (“the mummy,” “traffic”),and quarterback roth (“maleficent: mistress of evil ringed."
     ]
    }
   ],
   "source": [
    "# A5:SanityCheck\n",
    "\n",
    "j = 5\n",
    "document = newstweet[j]['text'].lower()\n",
    "np.random.seed(691)\n",
    "recitation, likelihood = recite(document, (ngram_frequencies, type_index, 0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ses2Pb2wHgoa"
   },
   "source": [
    "### 6. (2 pts) Build a perplexity performance evaluator\n",
    "We want to know how well this LM works, so let's compute the _average_ perplexity with respect to the document's stream of tokens (prediction points): $t_1, \\cdots, t_m$. For each $i=1,\\cdots,m$ of these, let $\\hat{y}_i\\in[0,1]^{|W|}$ be the probabilistic prediction vector over the vocabulary, so that $\\hat{y}_{i,t_i}$ is the prediction probability for the _correct_ type, $t_i$ at the $i^\\text{th}$ prediction point. Under this notation, we wish to compute the perplexity across our a document:\n",
    "$$\n",
    "\\mathcal{T}(t_1,\\cdots,t_m) = e^{\n",
    "    -\\frac{1}{m}\\sum_{i = 1}^m\\log{\\hat{y}_{i,t_i}}\n",
    "}\n",
    "$$\n",
    "In order to do this, we'll have to work from the `recite()` function's `likelihood` output format, which should now be a `list` of the $\\hat{y}_i\\in[0,1]^{|W|}$ values. \n",
    "\n",
    "With this all in mind, your job is to complete the `perplexity(likelihood)`, which returns a floating point number named `average_perplexity` (computed as above).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "5aSNl-iXHgoa"
   },
   "outputs": [],
   "source": [
    "# A6:Function(2/2)\n",
    "\n",
    "def perplexity(likelihood):\n",
    "    \n",
    "    #--- your code starts here\n",
    "    norm_coeff = 1/len(likelihood)\n",
    "    average_perplexity = np.exp(-norm_coeff*np.log(likelihood).sum())\n",
    "    #--- your code stops here\n",
    "    \n",
    "    return average_perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GLjqSMV6Hgoa"
   },
   "source": [
    "For reference, your output should be:\n",
    "```\n",
    "average perplexity of recitation:  1.8009612714103027\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "CCleGL9bHgoa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average perplexity of recitation:  1.8009546437644577\n"
     ]
    }
   ],
   "source": [
    "# A6:SanityCheck\n",
    "\n",
    "print(\"average perplexity of recitation: \", perplexity(likelihood))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EzzFk-rbHgob"
   },
   "source": [
    "### 7. (6 pts) Build a rambling function for the LM\n",
    "Here, our goal will be to have the LM 'ramble' from a given `prompt` of token-stream (list of strings) input. Note: while this function can go 'off the script', it must start from a prompt within its vocabulary, as specified within `LM_args[-1]`, i.e., `type_index` object. In particular, you must complete the function:\n",
    "```\n",
    "rambling, likelihood = ramble(prompt, docsize, LM_args, LM = P_next, n = 5, top = 1., verbose = True)\n",
    "```\n",
    "which accepts the following arguments:\n",
    "- `prompt`: a list of strings (tokens) which define the starting ngram for rambling prediction\n",
    "- `docsize`: the integer number of tokens to generate in the `rambling`\n",
    "- `LM_args`: same as for `recite()`\n",
    "- `LM (= P_next)`: same as for `recite()`\n",
    "- `n (= 5)`: same as for `recite()`\n",
    "- `top (= 1.)`: same as for `recite()`\n",
    "- `verbose (= True)`: same as for `recite()`\n",
    "\n",
    "and has the following return values are:\n",
    "- `rambling`: like `recitation`, a list of the tokens which the LM _predicts_, in order\n",
    "- `likelihood`: _now_, a list of the probabilities for the _predictions_ made by the LM as it operates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "LMpYpEZIHgob"
   },
   "outputs": [],
   "source": [
    "# A7:Function(6/6)\n",
    "\n",
    "def ramble(prompt, docsize, LM_args, LM = P_next, n = 5, top = 1., verbose = True):\n",
    "\n",
    "    if verbose:\n",
    "        print(\"generated document, starting from \\\"\"+\"\".join(prompt)+\"\\\":\\n\")\n",
    "    \n",
    "    likelihood = []; rambling = []\n",
    "    n1gram = prompt[-n:]\n",
    "    while len(rambling) < docsize:\n",
    "        \n",
    "        #--- your code starts here\n",
    "        Ps = LM(n1gram, *LM_args)\n",
    "        next_tok = sample_LM(n1gram, LM_args, top, LM)\n",
    "        likelihood.append(Ps[next_tok])\n",
    "        rambling.append(next_tok)\n",
    "        n1gram = tuple(list(n1gram[-n+1:]) + [next_tok])\n",
    "\n",
    "        #--- your code stops here\n",
    "        \n",
    "        if verbose:\n",
    "            print(rambling[-1], end = '')\n",
    "    return rambling, likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AKRH8GZgHgob"
   },
   "source": [
    "For reference, your output should be:\n",
    "```\n",
    "generated document, starting from \"robert downey jr\":\n",
    "\n",
    ". do with this issue, might also like:\n",
    "\n",
    "of course, not everything that excited the beautifully glowing phosphors of a high \n",
    "\n",
    "average perplexity of ramble:  1.6209028459433603\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "Xt_VqK8lHgob"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated document, starting from \"robert downey jr\":\n",
      "\n",
      ". do with this issue, might also like:\n",
      "\n",
      "of course, not everything that excited the beautifully glowing phosphors of a high \n",
      "\n",
      "average perplexity of ramble:  1.6209028459433603\n"
     ]
    }
   ],
   "source": [
    "# A7:SanityCheck\n",
    "\n",
    "j = 5\n",
    "np.random.seed(691)\n",
    "document = tokenize(newstweet[j]['text'].lower())\n",
    "rambling, likelihood = ramble(tuple(document[:5]), 46, \n",
    "                              (ngram_frequencies, type_index, 0.01))\n",
    "print(\"\\n\\naverage perplexity of ramble: \", perplexity(likelihood))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WmarO-O2Hgob"
   },
   "source": [
    "### 8. (4 pts) Constraining the vocabulary of a ramble\n",
    "Since we'd like to summarize these news articles, an easy trick to get the LM talk about the 'right stuff' is simply to constrain to the vocabulary of a given document. As such, we can and will make `type_index`-like objects for each article and then just use the same architecture as above.\n",
    "\n",
    "So here, you must complete the `make_doc_types()`, which accepts a list of strings named `documents`, the overall `type_index`, and a usual `space` boolean parameter. This amounts to constructing the `doc_types` object as a list of dictionaries, each of which has the same format as `type_index`, with the caveat, that each of `doc_types[j]` should only contain the type-index mapping for its given `j`th `document`, from `documents`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "w8KJS8VfHgob"
   },
   "outputs": [],
   "source": [
    "# A8:Function(4/4)\n",
    "\n",
    "def make_doc_types(documents, type_index, space = True):\n",
    "    \n",
    "    doc_types = []\n",
    "    #--- your code starts here\n",
    "    for doc in documents:\n",
    "        _, doc_unique_toks = make_ngram_frequency([doc])\n",
    "        doc_types.append({t: type_index[t] for t in doc_unique_toks})\n",
    "        \n",
    "    #--- your code stops here\n",
    "        \n",
    "    return doc_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IOyt0uu1Hgoc"
   },
   "source": [
    "For reference, your output should be:\n",
    "```\n",
    "generated document, starting from \"robert downey jr\":\n",
    "\n",
    ". and his wife and her friends as no people after the new england is nanjiani), an cynical ostrich (kumail nanjiani), an enthusiastic duck (octavia spencer), an enthusiastic duck (octavia spencer), an enthusiastic duck (octavia spencer), an upbeat polar bear (john cena), and all of the people that have the no. \n",
    "\n",
    "average perplexity of ramble:  2.26301542517039\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "wRQ0vDHwHgoc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated document, starting from \"robert downey jr\":\n",
      "\n",
      ". and his wife and her friends as no people after the new england is nanjiani), an cynical ostrich (kumail nanjiani), an enthusiastic duck (octavia spencer), an enthusiastic duck (octavia spencer), an enthusiastic duck (octavia spencer), an upbeat polar bear (john cena), and all of the people that have the no. \n",
      "\n",
      "average perplexity of ramble:  2.26301542517039\n"
     ]
    }
   ],
   "source": [
    "# A8:SanityCheck\n",
    "\n",
    "j = 5\n",
    "np.random.seed(691)\n",
    "document = tokenize(newstweet[j]['text'].lower())\n",
    "doc_types = make_doc_types([x['text'].lower() for x in newstweet], type_index)\n",
    "rambling, likelihood = ramble(tuple(document[:5]), 120, \n",
    "                              (ngram_frequencies, doc_types[j], 0.01))\n",
    "print(\"\\n\\naverage perplexity of ramble: \", perplexity(likelihood))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "A2-module-A.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
