{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","nbdir = \"/content/gdrive/My Drive/DSCI691/Assignment 3/module-A\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kPO-GyCWaP4d","executionInfo":{"status":"ok","timestamp":1651888086999,"user_tz":240,"elapsed":18423,"user":{"displayName":"Tien Nguyen","userId":"01698346109776197820"}},"outputId":"b79051ae-6040-4726-afe9-1aee65a51786"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","source":["%cd /content/gdrive/My Drive/DSCI691/Assignment 3/module-A"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x9pVZ1_r7ckV","executionInfo":{"status":"ok","timestamp":1651888088943,"user_tz":240,"elapsed":489,"user":{"displayName":"Tien Nguyen","userId":"01698346109776197820"}},"outputId":"93edcb6c-6997-4197-96db-55c4997be73b"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/My Drive/DSCI691/Assignment 3/module-A\n"]}]},{"cell_type":"markdown","metadata":{"id":"vT829vWKHgoL"},"source":["## Module submission header\n","### Submission preparation instructions \n","_Completion of this header is mandatory, subject to a 2-point deduction to the assignment._ Only add plain text in the designated areas, i.e., replacing the relevant 'NA's. You must fill out all group member Names and Drexel email addresses in the below markdown list, under header __Module submission group__. It is required to fill out descriptive notes pertaining to any tutoring support received in the completion of this submission under the __Additional submission comments__ section at the bottom of the header. If no tutoring support was received, leave NA in place. You may as well list other optional comments pertaining to the submission at bottom. _Any distruption of this header's formatting will make your group liable to the 2-point deduction._\n","\n","### Module submission group\n","- Group member 1\n","    - Name: Xi Chen\n","    - Email: xc98@drexel.edu\n","- Group member 2\n","    - Name: Tai Nguyen\n","    - Email: tdn47@drexel.edu\n","- Group member 3\n","    - Name: Tien Nguyen\n","    - Email: thn44@drexel.edu\n","- Group member 4\n","    - Name: Raymond Yung\n","    - Email: raymond.yung@drexel.edu\n","\n","\n","### Additional submission comments\n","- Tutoring support received: NA\n","- Other (other): We were not able to finish running A8 for sanity check because it took us hours each time running it. Therefore, we were not able to check A9, A10, A11. However, we tried our best to write the functions. "]},{"cell_type":"markdown","metadata":{"id":"3HEYAf_bjRJ6"},"source":["# DSCI 691: Natural language processing with deep learning <br> Assignment 3: GloVe semantic representation\n","## Data and Utilities \n","Here, we'll be working again with the same linked NewsTweet data, some essential utilities presented in the __Chapter 1 Notes__, as well as the adagrad optimization algorithm from the __Chapter 3 Notes__."]},{"cell_type":"code","execution_count":3,"metadata":{"id":"RfpwpFTS1ncL","executionInfo":{"status":"ok","timestamp":1651888097672,"user_tz":240,"elapsed":2739,"user":{"displayName":"Tien Nguyen","userId":"01698346109776197820"}}},"outputs":[],"source":["import json\n","newstweet = json.load(open('./data/newstweet-sample-linked.json'))\n","exec(open('./01-utilities.py').read())\n","exec(open('./03-utilities.py').read())"]},{"cell_type":"markdown","metadata":{"id":"pUcZOyTMHgoO"},"source":["## Overview \n","The purpose of this assignment (50 pts) is to gain some experience with iteration-based learning, which officially gets us into the DL work mode. Rather than the Word2Vec algorithm (based on language modeling), the GloVe algorithm's objective is to predict frequency. Let's see how this works!\n","\n","Note, there are several files which your code should produce, which to speed up your work will make it possible to work on later parts sooner. These are:\n","- a cached copy of the co-occurrence counts: `./data/cached/data-True-20-0.json`\n","- a cached copy of the training losses: `./data/cached/saved_losses_newstweet-GloVe-50-20_25000.json`\n","- a cached copy of the learned parameters: `./data/cached/saved_params_newstweet-GloVe-50-20_25000.npy`\n","- a cached copy of the sum of squared gradients: `./data/cached/saved_SSG_newstweet-GloVe-50-20_25000.npy`\n","- a cached copy of the model's state: `./data/cached/saved_state_newstweet-GloVe-50-20_25000.pickle`\n","\n","If you'd like to test later sections of code prior to earlier ones being completed, bring these files out of their `./data/cached/` directory and into the main `./data/` directory."]},{"cell_type":"markdown","metadata":{"id":"jiljr47SYUfl"},"source":["### 1. (3 pts) Build the model's training data\n","The [GloVe algorithm](https://nlp.stanford.edu/pubs/glove.pdf) relies on having access to the non-zero co-ocurrence values of a corpus to form the target variable for the regression task. Since it's cumbersome to construct and utilize these as a sparse matrix (and we don't want access to the zero-values anyway), it'll make sense to pre-compute these values and write them to disk. Hence, your first job is to complete the `make_co_counts(documents, space = True, k = 20, gamma = 0)` function, which\n","1. attempts to load pre-computed co-ocurrence counts (`co_counts`), or \n","2. computes them and store them to disk for future experimentation.\n","\n","So to complete the function, you'll have to construct it's return value, `co_counts`, which should be a `Counter()` of counts, keyed by a comma-separated string of the $i,j$ indices of the co-ocurring types from the _implied_ co-ocurrence matrix's index:\n","```\n","ij = \",\".join(map(str,[type_index[ti], type_index[tj]]))\n","```\n","Note: the `type_index` construction is already filled into to the function, and much of this computation should be essentially the same as for the construction of `co_counts` inside of the body of the `make_CoM` function."]},{"cell_type":"code","execution_count":4,"metadata":{"id":"GVBwAYTwYUfn","executionInfo":{"status":"ok","timestamp":1651888104135,"user_tz":240,"elapsed":184,"user":{"displayName":"Tien Nguyen","userId":"01698346109776197820"}}},"outputs":[],"source":["# A1:Function(3/3)\n","\n","import os\n","\n","def make_co_counts(documents, space = True, k = 20, gamma = 0):\n","    \n","    handle = \"-\".join(map(str,[space, k, gamma]))\n","    if os.path.exists('./data/data-' + handle + '.json'):\n","        return json.load(open('./data/data-' + handle + '.json'))\n","    \n","    document_frequency = Counter()\n","    for j, document in enumerate(documents):\n","        sentences = sentokenize(document.lower(), space = space)\n","        documents[j] = sentences\n","        frequency = Counter([t for s in documents[j] for t in s])\n","        document_frequency += Counter(frequency.keys())\n","    type_index = {t:i for i, t in enumerate(sorted(list(document_frequency.keys())))}\n","\n","    co_counts = Counter()\n","    \n","    #--- your code starts here\n","    for document in documents:\n","        for sentence in document:\n","            for i, ti in enumerate(sentence):\n","                context, weights = get_context(i, sentence, k = k, gamma = gamma)        \n","                for j, tj in enumerate(context):\n","                    ij = \",\".join(map(str,[type_index[ti], type_index[tj]]))\n","                    co_counts[ij] += weights[j]\n","    #--- your code stops here\n","\n","    data = {'co_counts': dict(co_counts), 'type_index': dict(type_index)}\n","    \n","    with open('./data/data-' + handle + '.json', \"w\") as f:\n","        f.write(json.dumps(data))\n","    \n","    return data"]},{"cell_type":"markdown","metadata":{"id":"lfYS3g_J64V2"},"source":["For reference, your output should be:\n","```\n","(111259, 15169286, 276685120.0)\n","```"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"iCIiU8e-64V3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651888129220,"user_tz":240,"elapsed":20381,"user":{"displayName":"Tien Nguyen","userId":"01698346109776197820"}},"outputId":"feae868d-3a72-4a39-a4e3-9abf1e31d549"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(111259, 15169286, 276685120.0)"]},"metadata":{},"execution_count":5}],"source":["# A1:SanityCheck\n","\n","data = make_co_counts([x['text'].lower() for x in newstweet])\n","len(data['type_index']), len(data['co_counts']), sum(data['co_counts'].values())"]},{"cell_type":"markdown","metadata":{"id":"S91nDlRR64V4"},"source":["### 2. (4 pts) Weight the co-occurrence matrix\n","Per the GloVe algorithm definition, we'll need to weight the terms of our loss function:\n","\n","$$\n","\\mathcal{L}= \\sum_{i = 1}^{|W|}\\sum_{j = 1}^{|W|}f(CoM_{i,j})\\left(\\vec{u}_i^T\\vec{v}_j + a_i + b_j - \\log{CoM_{i,j}}\\right)^{2}\n","$$\n","\n","By a weighting of the co-occurrence matrix, $CoM$, as:\n","$$\n","f(CoM_{i,j}) = 1\n","\\hspace{10pt}\\text{ if } \n","\\hspace{10pt}CoM_{i,j} \\geq CoM_{\\text{max}};\n","\\hspace{10pt}\\text{ otherwise } \n","\\hspace{10pt}f(CoM_{i,j}) = \\left(\\frac{CoM_{i,j}}{CoM_{\\text{max}}}\\right)^{\\alpha}.\n","$$\n","Generally, $\\alpha = 0.75$ and $CoM_{\\text{max}} = 100$ are recommended by the authors, but we'll leave these as presets for them as optional hyperparameters. \n","\n","In this part of the problem, your job is specifically to implement the $f$-weighting function for our $CoM$, below, by filling in the `weight_nonzero_data(data, comax = 100, alpha = 0.75)`. In particular, you should add a dctionary to the `data` object keyed as `data['fco_counts']`, which has `ij` keys (the co-ocurrence type-type comma-separated indices) corresponding to the weighted, $f(CoM_{i,j})$ values.\n","\n","Note: because this function is supposed to add a field to `data` it should have no return value."]},{"cell_type":"code","execution_count":6,"metadata":{"id":"VzboWaht64V5","executionInfo":{"status":"ok","timestamp":1651888161087,"user_tz":240,"elapsed":146,"user":{"displayName":"Tien Nguyen","userId":"01698346109776197820"}}},"outputs":[],"source":["# A2:Function(4/4)\n","\n","def weight_nonzero_data(data, comax = 100, alpha = 0.75):\n","    \n","    #--- your code starts here\n","    data['fco_counts'] = {ij: 1 if data['co_counts'][ij] >= comax \n","                          else ((data['co_counts'][ij])/comax)**alpha for ij in data['co_counts']}\n","\n","  \n","    #--- your code stops here\n","    \n","    # note: this function has no return value"]},{"cell_type":"markdown","metadata":{"id":"Df5yYOMs64V6"},"source":["For reference, your output should be:\n","```\n","(15169286, 1049012.2482885977)\n","```"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"yPnpJJ7-64V9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651888229462,"user_tz":240,"elapsed":19066,"user":{"displayName":"Tien Nguyen","userId":"01698346109776197820"}},"outputId":"9849deac-5ddf-4fab-9367-ebd39cfe949c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(15169286, 1049012.2482885977)"]},"metadata":{},"execution_count":7}],"source":["# A2:SanityCheck\n","\n","weight_nonzero_data(data)\n","len(data['fco_counts']), sum(data['fco_counts'].values())"]},{"cell_type":"markdown","metadata":{"id":"w8vfMfDP64V-"},"source":["### 3. (3 pts) Implement the loss function for an `ij` co-ocurrence pair\n","\n","Now that we have our co-ocurrence frequencies and weights, we can compute the GloVe loss function as:\n","\n","$$\n","\\mathcal{L}= \\sum_{i = 1}^{|W|}\\sum_{j = 1}^{|W|}f(CoM_{i,j})\\left(\\vec{u}_i^T\\vec{v}_j + a_i + b_j - \\log{CoM_{i,j}}\\right)^{2}\n","$$\n","\n","You job here is to implement the above math within the `GloVe_loss` function as `L` (which is the return value)."]},{"cell_type":"code","execution_count":8,"metadata":{"id":"CLWDN9v-64V-","executionInfo":{"status":"ok","timestamp":1651888229463,"user_tz":240,"elapsed":13,"user":{"displayName":"Tien Nguyen","userId":"01698346109776197820"}}},"outputs":[],"source":["# A3:Function(3/3)\n","\n","def GloVe_loss(ij, data, U, V, a, b):\n","    \n","    i, j = map(int, ij.split(','))\n","    \n","    #--- your code starts here\n","    import numpy as np\n","    L = data['fco_counts'][ij]*((np.dot(U[i], V[j]) + \n","                                 a[i] + b[j] - np.log(data['co_counts'][ij]))**2)\n","\n","    #--- your code stops here\n","    \n","    return L"]},{"cell_type":"markdown","metadata":{"id":"W-KWHy6m64V_"},"source":["For reference, your output should be:\n","```\n","3.069212443574228\n","```"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"NR1rEgR_64WA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651888229463,"user_tz":240,"elapsed":11,"user":{"displayName":"Tien Nguyen","userId":"01698346109776197820"}},"outputId":"9296edc3-3d42-42bc-c788-20e8bf61483d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["3.069212443574228"]},"metadata":{},"execution_count":9}],"source":["# A3:SanityCheck\n","\n","np.random.seed(691)\n","GloVe_loss(\",\".join(map(str, [data['type_index']['robert'], data['type_index']['downey']])), data, \n","           np.zeros((len(data['type_index']), 50)), np.zeros((len(data['type_index']), 50)), \n","                        np.zeros(len(data['type_index'])), np.zeros(len(data['type_index'])))"]},{"cell_type":"markdown","metadata":{"id":"THl8-g-C64WA"},"source":["### 4. (4 pts) Derive the gradient of the loss\n","Next, your job is to derive the gradients for the type $\\vec{u}_i$ and context $\\vec{v}_j$ vectors, in addition to for the bias terms, $a_i$ and $b_j$&mdash;to exhibit this work, fill in the steps you take to compute the gradients as markdown in the specified cells, below."]},{"cell_type":"markdown","metadata":{"id":"NIFKYliB64WB"},"source":["#### First, derive the scalar, partial-derivative bias terms:"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"dHuzpihW64WB","executionInfo":{"status":"ok","timestamp":1651888229464,"user_tz":240,"elapsed":10,"user":{"displayName":"Tien Nguyen","userId":"01698346109776197820"}}},"outputs":[],"source":["# A4:Derivation(1/4)"]},{"cell_type":"markdown","metadata":{"id":"OM9Nqb5664WC"},"source":["##### \\#\\#\\# your derivation starts here\n","$$\n","\\frac{\\partial{\\mathcal{L}}}{\\partial{a}_i} = \\sum_{i = 1}^{|W|}\\sum_{j = 1}^{|W|}2f(CoM_{i,j})\\left(\\vec{u}_i^T\\vec{v}_j + a_i + b_j - \\log{CoM_{i,j}}\\right)\n","$$\n","\n","$$\n","\\frac{\\partial{\\mathcal{L}}}{\\partial{b}_j} = \\sum_{i = 1}^{|W|}\\sum_{j = 1}^{|W|}2f(CoM_{i,j})\\left(\\vec{u}_i^T\\vec{v}_j + a_i + b_j - \\log{CoM_{i,j}}\\right)\n","$$\n","##### \\#\\#\\# your derivation stops here"]},{"cell_type":"markdown","metadata":{"id":"r7nyxVhV64WD"},"source":["#### Next, derive vector-gradients w/r to the $d$-dimensions of parameters:"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"WoUrl4r964WO","executionInfo":{"status":"ok","timestamp":1651888229465,"user_tz":240,"elapsed":10,"user":{"displayName":"Tien Nguyen","userId":"01698346109776197820"}}},"outputs":[],"source":["# A4:Derivation(3/4)"]},{"cell_type":"markdown","metadata":{"id":"H7QYAaWM64WP"},"source":["##### \\#\\#\\# your derivation starts here\n","$$\n","\\frac{\\partial\\mathcal{L}}{\\partial\\vec{u}_i} = \\sum_{i = 1}^{|W|}\\sum_{j = 1}^{|W|}2\\vec{v}_jf(CoM_{i,j})\\left(\\vec{u}_i^T\\vec{v}_j + a_i + b_j - \\log{CoM_{i,j}}\\right)\n","$$\n","\n","$$\n","\\frac{\\partial\\mathcal{L}}{\\partial\\vec{v}_j} = \\sum_{i = 1}^{|W|}\\sum_{j = 1}^{|W|}2\\vec{u}_if(CoM_{i,j})\\left(\\vec{u}_i^T\\vec{v}_j + a_i + b_j - \\log{CoM_{i,j}}\\right)\n","$$\n","##### \\#\\#\\# your derivation stops here"]},{"cell_type":"markdown","metadata":{"id":"IM-LT1mH64WP"},"source":["### 5. (6 pts) Implement a GloVe loss & gradient function\n","Using the results from your derivation above, implement the loss and gradient for all type and context embeddings. To do so, complete the `GloVe_loss_and_gradient` function, which has the following input _Arguments_:\n"," - `ij`: comma-separated (typed to strings) indices co-ocurring types, corresponding to a row from $V$ ($i$), $U$ ($j$), or index pair ($i,j$) from $CoM$ or $f(CoM)$.\n"," - `U`: the matrix of type vectors, i.e., rows of the matrix $U$ for all types\n"," - `V`: the matrix of context vectors, i.e., rows of the matrix $V$ for all contexts\n"," - `a`: the vector of bias terms for the types, $\\vec{a}$\n"," - `b`: the vector of bias terms for the contexts, $\\vec{b}$\n"," - `CoM`: the training data, co-occurrence matrix counting type-type 'context' pairs.\n"," - `fCoM`: the weighted training data of type-type 'context' pairs, $f(CoM)$.\n","\n","and the following _Return_ values:\n"," - `L`: the _scalar_ GLOVE loss function, $\\mathcal{L}$.\n"," - `dLdui`: the gradient _vector_ with respect to the specified type $\\frac{\\partial\\mathcal{L}}{\\partial\\vec{u}_i}$\n"," - `dLdvj`: the gradient _vector_ with respect to the specified context $\\frac{\\partial\\mathcal{L}}{\\partial\\vec{v}_j}$\n"," - `dai`: the _scalar_ partial derivative with respect to the type's bias parameter, $\\frac{\\partial{\\mathcal{L}}}{\\partial{a}_i}$\n"," - `dbj`: the _scalar_ partial derivative with respect to the context's bias parameter, $\\frac{\\partial{\\mathcal{L}}}{\\partial{b}_j}$\n"," \n","Note: the `return`ed loss and gradient values should be computed using your pre-defined function and derivation, above."]},{"cell_type":"code","execution_count":12,"metadata":{"id":"5uJGo6W964WQ","executionInfo":{"status":"ok","timestamp":1651888233870,"user_tz":240,"elapsed":160,"user":{"displayName":"Tien Nguyen","userId":"01698346109776197820"}}},"outputs":[],"source":["# A5:Function(6/6)\n","\n","def GloVe_loss_and_gradient(ij, data, U, V, a, b): \n","    \n","    i, j = map(int, ij.split(','))\n","    \n","    #--- your code starts here\n","    L = GloVe_loss(ij, data, U, V, a, b)\n","    dLdai = 2*data['fco_counts'][ij]*(np.dot(U[i], V[j]) + \n","                                 a[i] + b[j] - np.log(data['co_counts'][ij]))\n","    dLdbj = dLdai\n","    dLdui = V[j]*dLdai\n","    dLdvj = U[i]*dLdai\n","    \n","    \n","    #--- your code stops here\n","    \n","    return L, dLdui, dLdvj, dLdai, dLdbj"]},{"cell_type":"markdown","metadata":{"id":"X9_I3ha664WR"},"source":["For reference, your output should be:\n","```\n","(3.069212443574228,\n"," array([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n","        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n","        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n","        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.]),\n"," array([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n","        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n","        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n","        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.]),\n"," -1.9858753089849215,\n"," -1.9858753089849215)\n","```"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"5LJUcmxg64WR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651888241303,"user_tz":240,"elapsed":167,"user":{"displayName":"Tien Nguyen","userId":"01698346109776197820"}},"outputId":"7ba0e1b1-480d-4237-a6c9-628655c79505"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(3.069212443574228,\n"," array([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n","        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n","        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n","        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.]),\n"," array([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n","        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n","        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n","        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.]),\n"," -1.9858753089849215,\n"," -1.9858753089849215)"]},"metadata":{},"execution_count":13}],"source":["# A5:SanityCheck\n","\n","np.random.seed(691)\n","GloVe_loss_and_gradient(\",\".join(map(str, [data['type_index']['robert'], data['type_index']['downey']])), data, \n","                        np.zeros((len(data['type_index']), 50)), np.zeros((len(data['type_index']), 50)), \n","                        np.zeros(len(data['type_index'])), np.zeros(len(data['type_index'])))"]},{"cell_type":"markdown","metadata":{"id":"H_soEVos64WS"},"source":["### 6. (4 pts) Building a sampling function\n","Here, you're required to build out the `sample_nonz(batch_size, ijs)` function, which must sample `batch_size` pairs of ($i,j$)-indices that are keys for the non-zero values from the $CoM$ within `data['co_counts']`. This will make it efficient to iterate over non-trivial portions of the co-occurrence matrix. \n","\n","Note: for the trivial case, when `batch_size == len(ijs)`, your function should just return a copy of `ijs`, and in order to match the `# A6:SanityCheck` output, build your code by using the `random` module's `sample` function, which can be accessed via this assignment's namespace as `ra.sample`."]},{"cell_type":"code","execution_count":14,"metadata":{"id":"F13ypsKG64WT","executionInfo":{"status":"ok","timestamp":1651888244175,"user_tz":240,"elapsed":174,"user":{"displayName":"Tien Nguyen","userId":"01698346109776197820"}}},"outputs":[],"source":["# A6:Function(4/4)\n","\n","def sample_ijs(batch_size, ijs):\n","    \n","    #--- your code starts here\n","    \n","    if batch_size == len(ijs):\n","      ijs_sample = ijs\n","    else:\n","      ijs_sample = [item for item in ra.sample( ijs, batch_size)]\n","    #--- your code stops here\n","    \n","    ra.shuffle(ijs_sample)\n","    return ijs_sample"]},{"cell_type":"markdown","metadata":{"id":"YHFxLMQk64WU"},"source":["For reference, your output should be:\n","```\n","['40084,14097',\n"," '71564,105579',\n"," '56945,14591',\n"," '110912,95636',\n"," '38321,38439',\n"," '56064,108528',\n"," '89963,59086',\n"," '100840,33233',\n"," '33494,108283',\n"," '99008,94653']\n","```"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"kozOzI1H64WU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651888248197,"user_tz":240,"elapsed":513,"user":{"displayName":"Tien Nguyen","userId":"01698346109776197820"}},"outputId":"4196cf35-9098-4acd-e15d-1cdfd733f4f0"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['40084,14097',\n"," '71564,105579',\n"," '56945,14591',\n"," '110912,95636',\n"," '38321,38439',\n"," '56064,108528',\n"," '89963,59086',\n"," '100840,33233',\n"," '33494,108283',\n"," '99008,94653']"]},"metadata":{},"execution_count":15}],"source":["# A6:SanityCheck\n","\n","ra.seed(691)\n","sample_ijs(10, list(data['co_counts'].keys()))"]},{"cell_type":"markdown","metadata":{"id":"dhG521hf64WV"},"source":["### 7. (6 pts) Operating GloVe on batches of `ij` index-pairs\n","Here, we'll produce the primary `GloVe` function, which computes gradient components as prescribed by our non-zero $CoM$-entry-sampling function and a given `batch_size`. This function has the following input _Arguments_:\n","- `UVab`: the current model parameters for the GloVe algorithm\n","- `data`: the training data, continaing co-occurrence counts keyed by `'co_counts'` and weighted frequency, keyed by `'fco_counts'`, in addition to the type index, keyed by `'type_index'`.\n","- `batch_size`: the number of word-context pairs to process into gradients at once\n","- `loss_and_gradient`: the loss and gradient function for GloVe\n","\n","and the following _Return_ values:\n","- `total_L`: the loss function value for the skip-gram model ($\\mathcal{L}$)\n","- `gradient`: the stacked gradient vectors in the `UVab` order\n","\n","Your job is to aggregate the gradient components into their correct portions of the matrix representation named `gradient`. In particular, for each `ij` co-ocurrence index pair, the gradient output (`dLduj, dLdvi, dLdai, dLdbj`) from the `loss_and_gradient` function must be added to the `gradient` matrix derivatives in the same locations corresponding to the $i,j$ parameters from $U$, $V$, $a$, and $b$, respectively. \n","\n","Additionally, your code must aggregate into `total_weight` the value from all `ij` co-occurrence index pairs."]},{"cell_type":"code","execution_count":16,"metadata":{"id":"RygAS35G64WV","executionInfo":{"status":"ok","timestamp":1651888253193,"user_tz":240,"elapsed":175,"user":{"displayName":"Tien Nguyen","userId":"01698346109776197820"}}},"outputs":[],"source":["# A7:Function(6/6)\n","\n","def GloVe(UVab, data, batch_size = 0, loss_and_gradient = GloVe_loss_and_gradient):\n","    dim = int((UVab.shape[1]/2) - 1)\n","    ## zero batch_size means compute over whole dataset\n","    if not batch_size:\n","        batch_size = len(co_counts)\n","    U = UVab[:,:dim]; V = UVab[:,dim:2*dim]\n","    a = UVab[:,-2]; b = UVab[:,-1]\n","    gradient = np.zeros(UVab.shape)\n","    total_weight = 0; total_L = 0.0\n","    for ij in sample_ijs(batch_size, list(data['co_counts'].keys())):\n","        \n","        i, j = map(int, ij.split(','))\n","        \n","        #--- your code starts here\n","        L, dLdui, dLdvj, dLdai, dLdbj = loss_and_gradient(ij, data, U, V, a, b)\n","        total_L += L\n","        gradient[i, :dim] = dLdui\n","        gradient[:,dim:2*dim] = dLdvj\n","        gradient[:,-2] = dLdai\n","        gradient[:, -1] = dLdbj\n","        total_weight += data['fco_counts'][ij]\n","        #--- your code stops here\n","        \n","    total_L, gradient = total_L/total_weight, gradient/total_weight\n","    return total_L, gradient"]},{"cell_type":"markdown","metadata":{"id":"R_WFpBGh64WV"},"source":["For reference, your output should be:\n","```\n","(3.5667257586946493,\n"," array([[0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        ...,\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.]]))\n","```"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"f-r77LQV64WW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651888266543,"user_tz":240,"elapsed":2113,"user":{"displayName":"Tien Nguyen","userId":"01698346109776197820"}},"outputId":"c4bf42d6-c338-47bd-e734-65d12f1aaa72"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(3.5667257586946493, array([[0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        ...,\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.]]))"]},"metadata":{},"execution_count":17}],"source":["# A7:SanityCheck\n","\n","ra.seed(691)\n","np.random.seed(691)\n","GloVe(np.zeros((len(data['type_index']), (50+1)*2)), data, batch_size = 100)"]},{"cell_type":"markdown","metadata":{"id":"DVbWP71s64WY"},"source":["### 8. (4 pts) Complete the GloVe training wrapper\n","Now it's time to complete the GloVe implementation by filling in the `train` function, which operates much like that from __Chapter 3, Section 3.1.1.6__, which also uses the adaptive gradient descent algorithm for implementation. You'll have access to this again via the code from `03-utilities.py`. \n","\n","In particular, use the pre-set model initial random state:\n","$$\n","UVab_0\\in[-0.5,0.5]^{|W|\\times 2(d+1)},\n","$$\n","which stacks $U$ $V$ $a$ and $b$ as columns in a matrix with values from $[-1,1]$ to apply the `adagrad()` function by passing the `GloVe` model as a lambda function with variable argument as the model parameters, i.e., an 'anonymous' matrix variable `UVab`.\n","\n","When applying the `adagrad` function, be sure to supply the correct parameters from the `train()` function's input, in addition to catching the `UVab_m, losses` output that it returns (the final result from `m` iterations).\n","\n","Note: your code should consist only of a single function call&mdash;to the `adagrad()` function."]},{"cell_type":"code","execution_count":18,"metadata":{"id":"E3SE-hY764Wa","executionInfo":{"status":"ok","timestamp":1651888274385,"user_tz":240,"elapsed":151,"user":{"displayName":"Tien Nguyen","userId":"01698346109776197820"}}},"outputs":[],"source":["# A8:Function(4/4)\n","\n","import time\n","def train(data, handle, k = 20, dim = 50, comax = 100, alpha = 0.75, \n","          batch_size = 100, m = 25000, eta = 0.5,\n","          use_saved = True, save_every = 1000, print_every=100,\n","          loss_and_gradient = GloVe_loss_and_gradient):\n","    \n","    N = len(data['type_index'])\n","    handle = \"-\".join([handle, str(dim), str(k)])\n","    ra.seed(691);np.random.seed(691)\n","    \n","    start_time=time.time()\n","    \n","    UVab_0 = np.concatenate(((np.random.rand(N, dim + 1) - 0.5)/(dim + 1),\n","                             (np.random.rand(N, dim + 1) - 0.5)/(dim + 1)), axis=1)\n","    \n","    #--- your code starts here\n","    UVab_m, losses = adagrad(lambda UVab: GloVe(UVab, data, batch_size = batch_size),\n","                             UVab_0, eta, m, handle, use_saved, save_every, print_every)\n","    #--- your code stops here\n","    \n","    stop_time=time.time()\n","    return UVab_m, losses, (start_time, stop_time)"]},{"cell_type":"markdown","metadata":{"id":"-Id-iFLAxfPf"},"source":["For reference, your output should be:\n","\n","```\n","...\n","iteration:  100 avg loss up to batch:  4.8556129373186545\n","...\n","iteration:  500 avg loss up to batch:  3.268102535178591\n","...\n","iteration:  1000 avg loss up to batch:  2.700337325161406\n","...\n","iteration:  2000 avg loss up to batch:  2.2696841678746846\n","...\n","iteration:  3000 avg loss up to batch:  2.060780232568986\n","...\n","iteration:  4000 avg loss up to batch:  1.937148112389848\n","...\n","iteration:  5000 avg loss up to batch:  1.8688602231891303\n","...\n","iteration:  10000 avg loss up to batch:  1.700315477882425\n","...\n","iteration:  15000 avg loss up to batch:  1.6058466121965451\n","...\n","iteration:  20000 avg loss up to batch:  1.5143681294461326\n","...\n","iteration:  25000 avg loss up to batch:  1.432184821156592\n","training time (in hours):  <3–6, depending on system>\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zQBZPdk164Wa","scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"outputId":"d232f748-4a70-4057-b92c-850913f8e84d"},"outputs":[{"output_type":"stream","name":"stdout","text":["iteration:  11100 avg loss up to batch:  3.2584514900783565\n","iteration:  11200 avg loss up to batch:  3.2552854689140363\n","iteration:  11300 avg loss up to batch:  3.254052727689305\n","iteration:  11400 avg loss up to batch:  3.2540302338073013\n","iteration:  11500 avg loss up to batch:  3.2510969610523253\n","iteration:  11600 avg loss up to batch:  3.2475417875641943\n","iteration:  11700 avg loss up to batch:  3.2492006878278397\n","iteration:  11800 avg loss up to batch:  3.24846168095704\n","iteration:  11900 avg loss up to batch:  3.2484763135324894\n","iteration:  12000 avg loss up to batch:  3.247301684025125\n","iteration:  12100 avg loss up to batch:  3.2464563177376906\n","iteration:  12200 avg loss up to batch:  3.247560358801099\n","iteration:  12300 avg loss up to batch:  3.245473802247293\n","iteration:  12400 avg loss up to batch:  3.245634459712037\n","iteration:  12500 avg loss up to batch:  3.2452445361177826\n","iteration:  12600 avg loss up to batch:  3.243407523405031\n","iteration:  12700 avg loss up to batch:  3.243746082237667\n","iteration:  12800 avg loss up to batch:  3.2450443073858675\n","iteration:  12900 avg loss up to batch:  3.246612511553699\n","iteration:  13000 avg loss up to batch:  3.249119822612206\n","iteration:  13100 avg loss up to batch:  3.247712386607981\n","iteration:  13200 avg loss up to batch:  3.2448486778702534\n","iteration:  13300 avg loss up to batch:  3.242239896751081\n","iteration:  13400 avg loss up to batch:  3.241750803762084\n","iteration:  13500 avg loss up to batch:  3.238154060457563\n","iteration:  13600 avg loss up to batch:  3.2342949898663216\n"]}],"source":["# A8:SanityCheck\n","\n","UVab, losses, tt = train(data, 'newstweet-GloVe')\n","print(\"training time (in hours): \", (tt[1] - tt[0])/(60*60))"]},{"cell_type":"markdown","metadata":{"id":"y5dJiGw_xfPg"},"source":["### 9. (6 pts) Build an analogy tester\n","Now that we've got some vectors (even if under-trained), let's go ahead with [Mikolov's analogy test](https://arxiv.org/pdf/1301.3781.pdf), which checks to see if SAT-like analogies can be completed via cosine similarity comparison of vectors. For example, given the usual analogy: \"\n","> king is to man as woman is to queen\n","\n","let's refer to `('man', 'king')` as the known `pair`, `woman` as the uknown's `predicate`, and `queen` as the unknown's `target` type. According to the word analogy test, 'good' word vectors will satisfy the relationship:\n","$$\n","\\hat{v} = v_\\text{king} - v_\\text{man} + v_\\text{woman} \\sim v_\\text{queen}\n","$$\n","\n","Where $\\sim$ is specifically measured as the cosine similarity. \n","\n","Your job is build this evaluation for a given set of vectors, `X`, and their `type_index`, and a specific analogy, provided to the `test_analogy` function as arguments, named `pair`, `predicate`, and `target`. As output, your function must compute the `rank` of the `target` type, according to the sorted set, decreasing, by cosine similarity. \n","\n","Also, there's a catch&mdash;make sure your output filters both `pair` terms and the `predicate` _before_ ranking, otherwise your performance will be underestimated!!\n","\n","Note: please use the `sklearn.metrics.pairwise.cosine_similarity` implementation of the cosine similarity in order to match output with the provided sample. Sorting issues have been observed from standard numpy matrix operations."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CjypkCJH64Wc"},"outputs":[],"source":["# A9:Function(6/6)\n","\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","def test_analogy(pair, predicate, target, type_index, X):\n","\n","    #--- your code starts here\n","    vec = X/np.linalg.norm(X, axis = 1)[:, np.newaxis]\n","    vp1 = vec[type_index[pair[0]], :]\n","    vp2 = vec[type_index[pair[1]], :]\n","    vpred = vec[type_index[predicate], :]\n","    vtarget = vec[type_index[target], :]\n","    \n","    vhat = vp2 - vp1 + vpred\n","    target_sim = cosine_similarity([vhat], [vtarget])[0][0]\n","    \n","    sims = []\n","    for token in type_index:\n","        v_token = vec[type_index[token], :]\n","        token_sim = cosine_similarity([vhat], [vtarget])[0][0]\n","        sims.append(token_sim)\n","        \n","    sims.sort(reverse=True)\n","    \n","    for i, sim in enumerate(sims):\n","        if sim == target_sim:\n","            rank = i\n","    #--- your code stops here\n","\n","    return rank, sim"]},{"cell_type":"markdown","metadata":{"id":"lgsGsaWaxfPh"},"source":["Note: the `# A9:SanityCheck` below tests agains the different components of your trained vectors, in addition to those from the `CoM` model (measured from the larger data set) _and_ the set 50-dimensional GloVe pretrained \"Wikipedia 2014 + Gigaword 5\" vectors, [obtained from the author's original resource page](https://nlp.stanford.edu/projects/glove/).\n","\n","For reference, your output should be:\n","```\n","[('U rank, similarity: ', (27, 0.9873522387316712)),\n"," ('V rank, similarity: ', (554, 0.966891580621855)),\n"," ('UVab rank, similarity: ', (31, 0.9804160087551583)),\n"," ('CoM rank, similarity: ', (9, 0.9996402262687847)),\n"," ('pretrained rank, similarity: ', (1, 0.9373217383382935))]\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"spPE8NWIxfPh"},"outputs":[],"source":["# A9:SanityCheck\n","\n","# let's break down our vectors to see how a few different pieces perform\n","dim = int((UVab.shape[1]/2) - 1)\n","U = UVab[:,:dim]; V = UVab[:,dim:2*dim]\n","\n","# load some CoM statistics from the larger file\n","CoM_d = np.load(\"./data/newstweet-sample-linked-CoM_d.npy\")\n","CoM_d_index = json.load(open(\"./data/newstweet-sample-linked-type_index.json\"))\n","\n","# load the pre-trained 50-dimensional wikipedia GloVe vectors\n","import csv\n","GloVe_pretrained = []; GloVe_pretrained_index = {}\n","for line in open('./data/glove.6B.50d.txt'):\n","    row = line.split(\" \")\n","    if row[0] in data['type_index']:\n","        GloVe_pretrained.append(list(map(float, row[1:])))\n","        GloVe_pretrained_index[row[0]] = len(GloVe_pretrained_index)\n","    \n","GloVe_pretrained = np.array(GloVe_pretrained)\n","\n","[(\"U rank, similarity: \", test_analogy(('man', 'he'), 'woman', 'she', data['type_index'], U)),\n"," (\"V rank, similarity: \", test_analogy(('man', 'he'), 'woman', 'she', data['type_index'], V)),\n"," (\"UVab rank, similarity: \", test_analogy(('man', 'he'), 'woman', 'she', data['type_index'], UVab)),\n"," (\"CoM rank, similarity: \", test_analogy(('man', 'he'), 'woman', 'she', CoM_d_index, CoM_d)),\n"," (\"pretrained rank, similarity: \", test_analogy(('man', 'he'), 'woman', 'she', GloVe_pretrained_index, GloVe_pretrained))]"]},{"cell_type":"markdown","metadata":{"id":"k95azUzAxfPh"},"source":["### 10. (6 pts) Test a sample of analogies using the tester\n","The `analogies` we'll be using are the [MSR](https://www.microsoft.com/en-us/research/people/) set (see [here](https://aclweb.org/aclwiki/Analogy_(State_of_the_art)) for more details) and will be loaded as a dataframe of shape:\n","```\n",">>> print(analogies.head())\n","   Unnamed: 0     type   word1   word2     word3    target\n","0           0   JJ_JJR    good  better     rough   rougher\n","1           1   JJR_JJ  better    good   rougher     rough\n","2           2   JJ_JJS    good    best     rough  roughest\n","3           3   JJS_JJ    best    good  roughest     rough\n","4           4  JJS_JJR    best  better  roughest   rougher\n","```\n","i.e., so that by column: `pair = (word1, word2)`, `predicate = word3`, and `target = target`.\n","\n","Using these data, form the input needed and apply the `test_analogy` function. Then, use its `rank`  ($r$) output to compute a `score` as:\n","$$\n","\\text{score} = 1 - \\frac{r - 1}{|W|}\n","$$\n","Each computed `rank` and `score` should be appended into its respective list, and the scores will provide a baseline idea of the model's performance against random guessing, i.e., whereupon the list of `scores` should average to $\\approx 0.5$. We'll use the `ranks` to compute another ranking metric in the next, too.\n","\n","Note: you must make sure that _all_ components of the analogy exist in the `type_index` before attempting to `rank` and `score`, otherwise, ignore the analogy!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HyeGo0ID64Wh"},"outputs":[],"source":["# A10:Function(6/6)\n","\n","def analyze_analogies(analogies, type_index, X, num = 250, verbose = True):\n","    scores = []; ranks = []\n","    for i, row in analogies.sample(n=analogies.shape[0], random_state=691).iterrows():\n","\n","        #--- your code starts here\n","        if (row['word1'] and row['word2'] and row['word3'] and row['target']) in type_index:\n","          pair = (row['word1'], row['word2'])\n","          predicate = row['word3']\n","          target = row['target']\n","\n","          rank, sim = test_analogy(pair, predicate, target, type_index, X)\n","\n","          score = 1 - (rank-1)/len(type_index)\n","\n","          ranks.append(rank)\n","          scores.append(score)\n","        #--- your code stops here\n","        \n","        if verbose and not len(scores) % int(num/10):\n","            print(100*len(scores)/num, \"% complete\")\n","        \n","        if len(scores) == num:\n","            break\n","    return scores, ranks"]},{"cell_type":"markdown","metadata":{"id":"FCu6jlU5xfPi"},"source":["For reference, your output should be:\n","```\n","Analyzing U-matrix output...\n","10.0 % complete\n","20.0 % complete\n","30.0 % complete\n","40.0 % complete\n","50.0 % complete\n","60.0 % complete\n","70.0 % complete\n","80.0 % complete\n","90.0 % complete\n","100.0 % complete\n","done. total score:  172.67554085512182\n","Analyzing V-matrix output...\n","10.0 % complete\n","20.0 % complete\n","30.0 % complete\n","40.0 % complete\n","50.0 % complete\n","60.0 % complete\n","70.0 % complete\n","80.0 % complete\n","90.0 % complete\n","100.0 % complete\n","done. total score:  175.3225446930135\n","Analyzing UVab-matrix output...\n","10.0 % complete\n","20.0 % complete\n","30.0 % complete\n","40.0 % complete\n","50.0 % complete\n","60.0 % complete\n","70.0 % complete\n","80.0 % complete\n","90.0 % complete\n","100.0 % complete\n","done. total score:  186.3084334750446\n","Analyzing CoM-matrix output...\n","10.0 % complete\n","20.0 % complete\n","30.0 % complete\n","40.0 % complete\n","50.0 % complete\n","60.0 % complete\n","70.0 % complete\n","80.0 % complete\n","90.0 % complete\n","100.0 % complete\n","done. total score:  193.1615869277991\n","Analyzing pretrained-matrix output...\n","10.0 % complete\n","20.0 % complete\n","30.0 % complete\n","40.0 % complete\n","50.0 % complete\n","60.0 % complete\n","70.0 % complete\n","80.0 % complete\n","90.0 % complete\n","100.0 % complete\n","done. total score:  249.48278079549016\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QxU3PRvIxfPi","scrolled":true},"outputs":[],"source":["# A10:SanityCheck\n","\n","import pandas as pd\n","analogies = pd.read_csv('./data/msr.csv')\n","\n","print(\"Analyzing U-matrix output...\")\n","U_scores, U_ranks = analyze_analogies(analogies, data['type_index'], U)\n","print(\"done. total score: \", sum(U_scores))\n","\n","print(\"Analyzing V-matrix output...\")\n","V_scores, V_ranks = analyze_analogies(analogies, data['type_index'], V)\n","print(\"done. total score: \", sum(V_scores))\n","\n","print(\"Analyzing UVab-matrix output...\")\n","UVab_scores, UVab_ranks = analyze_analogies(analogies, data['type_index'], UVab)\n","print(\"done. total score: \", sum(UVab_scores))\n","\n","print(\"Analyzing CoM-matrix output...\")\n","CoM_scores, CoM_ranks = analyze_analogies(analogies, CoM_d_index, CoM_d)\n","print(\"done. total score: \", sum(CoM_scores))\n","\n","print(\"Analyzing pretrained-matrix output...\")\n","pretrained_scores, pretrained_ranks = analyze_analogies(analogies, GloVe_pretrained_index, GloVe_pretrained)\n","print(\"done. total score: \", sum(pretrained_scores))"]},{"cell_type":"markdown","metadata":{"id":"SMxB07EHxfPi"},"source":["### 11. (4 pts) Test a sample of analogies using the tester\n","Finally, to complete the assignmen your job is to produce a function that computes aggregated performance statistics for the a given list of `scores` and `ranks`. In particular, your code must compute \n","1. the arithmetic `mean_score`, i.e., simple arithmetic average of the `scores`;\n","2. the model's overall `accuracy`, i.e., the number of analogies of rank `1`; and\n","3. the model's [mean reciprocal rank (MRR)](https://en.wikipedia.org/wiki/Mean_reciprocal_rank), which is the reciprocal of the harmonic mean of ranks."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RFi4fMXQxfPj"},"outputs":[],"source":["# A11:Function(4/4)\n","\n","def evaluate_performance(scores, ranks):\n","    \n","    #--- your code starts here\n","    mean_score = np.average(scores)\n","    accuracy = 100*np.sum([1 for rank in ranks if rank ==1 ])/len(scores)\n","    MRR = (1/len(ranks)* np.sum(1/np.array(ranks))\n","\n","\n","    #--- your code stops here\n","    \n","    return mean_score, accuracy, MRR"]},{"cell_type":"markdown","metadata":{"id":"HJM1G2trxfPj"},"source":["For reference, your output should be:\n","```\n","U performance (mean score, accuracy, MRR):  (0.6907021634204873, 0.0, 0.001740256245558779)\n","V performance (mean score, accuracy, MRR):  (0.701290178772054, 0.0, 0.0009884626617602233)\n","UVab performance (mean score, accuracy, MRR):  (0.7452337339001784, 0.0, 0.0024205505157109185)\n","CoM performance (mean score, accuracy, MRR):  (0.7726463477111964, 0.4, 0.010905247640529447)\n","pre-trained performance (mean score, accuracy, MRR):  (0.9979311231819606, 40.0, 0.5068096534223601)\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"79xHXLytxfPj"},"outputs":[],"source":["# A11:SanityCheck\n","\n","# test the NewsTweet-trained GloVe model U component\n","print(\"U performance (mean score, accuracy, MRR): \", \n","      evaluate_performance(U_scores, U_ranks))\n","\n","# test the NewsTweet-trained GloVe model V component\n","print(\"V performance (mean score, accuracy, MRR): \", \n","      evaluate_performance(V_scores, V_ranks))\n","\n","# test the NewsTweet-trained full GloVe model\n","print(\"UVab performance (mean score, accuracy, MRR): \", \n","      evaluate_performance(UVab_scores, UVab_ranks))\n","\n","# test the NewsTweet-trained CoM model\n","print(\"CoM performance (mean score, accuracy, MRR): \", \n","      evaluate_performance(CoM_scores, CoM_ranks))\n","\n","# test the Wikipedia pre-trained 50-d GloVe model\n","print(\"pre-trained performance (mean score, accuracy, MRR): \", \n","      evaluate_performance(pretrained_scores, pretrained_ranks))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HxvGjJcexfPj"},"outputs":[],"source":[""]}],"metadata":{"colab":{"collapsed_sections":[],"name":"A3-module-A.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"nbformat":4,"nbformat_minor":0}