{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkQDuZm9pIiJ"
      },
      "source": [
        "## Module submission header\n",
        "### Submission preparation instructions \n",
        "_Completion of this header is mandatory, subject to a 2-point deduction to the assignment._ Only add plain text in the designated areas, i.e., replacing the relevant 'NA's. You must fill out all group member Names and Drexel email addresses in the below markdown list, under header __Module submission group__. It is required to fill out descriptive notes pertaining to any tutoring support received in the completion of this submission under the __Additional submission comments__ section at the bottom of the header. If no tutoring support was received, leave NA in place. You may as well list other optional comments pertaining to the submission at bottom. _Any distruption of this header's formatting will make your group liable to the 2-point deduction._\n",
        "\n",
        "### Module submission group\n",
        "- Group member 1\n",
        "    - Name: NA\n",
        "    - Email: NA\n",
        "- Group member 2\n",
        "    - Name: NA\n",
        "    - Email: NA\n",
        "- Group member 3\n",
        "    - Name: NA\n",
        "    - Email: NA\n",
        "- Group member 4\n",
        "    - Name: NA\n",
        "    - Email: NA\n",
        "\n",
        "### Additional submission comments\n",
        "- Tutoring support received: NA\n",
        "- Other (other): NA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HEYAf_bjRJ6"
      },
      "source": [
        "# DSCI 691: Natural language processing with deep learning <br> Assignment 1: Extracting Tweet-Like Summaries from the News\n",
        "## Data and Utilities \n",
        "Here, we'll be working with the same linked NewsTweet data and some essential utilities presented in the __Chapter 1 Notes__."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RfpwpFTS1ncL"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "newstweet = json.load(open('./data/newstweet-subsample-linked.json'))\n",
        "exec(open('./01-utilities.py').read())\n",
        "# from utilities import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uk1tzOsupaxA"
      },
      "source": [
        "## Overview \n",
        "The purpose of this assignment (45 pts) is to gain some experience with text statistical variation, and likewise, to explore how it can be decoded for a given task and what can be done with the subsequently-learned information. \n",
        "\n",
        "At a high level, the NewsTweet tweet prediction task's specific purpose was the determine if a given article contains a tweet. But if we think about applying this to out-of-set data as 'test' documents, what does it tell us?\n",
        "\n",
        "Well, since we know tweets have limited size, one thought might be:\n",
        "\n",
        "> Can we use the CBOW classifier to discover where the tweet(s) are in the articles?\n",
        "\n",
        "Satisfying this task would be a kind of _extractive summarization_ task, in which the purpose is to predict a text span as being representative of some annotation. For us, there is certainly a portion of the text that represents our 'has tweet' annotation, at it is _any_ of the tweet-text spans that a given article contains. \n",
        "\n",
        "Let's see what CBOW can find!\n",
        "## Experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiljr47SYUfl"
      },
      "source": [
        "### 1. (2 pts) Operate the Chapter 1 Statistical Engine\n",
        "Here, you must complete the `train_cbow(newstweet)` function, which entails operating the `make_TDM`, `make_CoM`, `svdsub`, `cbow`, and `eval_tweet_prediction` functions under the default settings over all `newstweet` `'text'` fields, and storing their outputs.\n",
        "\n",
        "In particular, you _must_ recover the following named objects:\n",
        "\n",
        "- a `TDM` from the `make_TDM` function;\n",
        "- a `CoM` from the `make_CoM` function;\n",
        "- the `CoM_d` dimensional reduction from `svdsub`;\n",
        "- the `CDM` from `CBOW` on the dimensionally reduced semantic vectors;\n",
        "- a `type_index` from _one of_ `make_TDM` or `make_CoM`; _and_\n",
        "- the `result`, `classifier`, and `threshold` produced from `eval_tweet_prediction` run with the supplied `state = 0` argument."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "GVBwAYTwYUfn"
      },
      "outputs": [],
      "source": [
        "# A1:Function(2/2)\n",
        "\n",
        "\n",
        "def train_cbow(newstweet):\n",
        "    state = 0\n",
        "\n",
        "    # --- your code starts here\n",
        "    # do_tfidf = True\n",
        "    # space = True\n",
        "    # k = 20\n",
        "    # d = 50\n",
        "    # gamma = 0\n",
        "\n",
        "    TDM, type_index = make_TDM([x[\"text\"] for x in newstweet])\n",
        "    CoM, type_index = make_CoM([x[\"text\"] for x in newstweet])\n",
        "    CoM_d = svdsub(CoM)\n",
        "    CDM = cbow(TDM, CoM_d)\n",
        "\n",
        "    result, classifier, threshold = eval_tweet_prediction(CDM, newstweet, state=state)\n",
        "    # --- your code stops here\n",
        "\n",
        "    return TDM, type_index, CoM, CoM_d, CDM, result, classifier, threshold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUoOnyoUwGVl"
      },
      "source": [
        "For reference, your output should be:\n",
        "```\n",
        "Counter({'F1': 0.4324324324324324, 'P': 0.47058823529411764, 'R': 0.4})\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "crc7Sf8wuXHi"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<string>:69: RuntimeWarning: divide by zero encountered in true_divide\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Counter({'P': 0.4090909090909091, 'R': 0.45, 'F1': 0.4285714285714286})"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# A1:SanityCheck\n",
        "\n",
        "TDM, type_index, CoM, CoM_d, CDM, result, classifier, threshold = train_cbow(newstweet)\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "UsageError: Line magic function `%` not found.\n"
          ]
        }
      ],
      "source": [
        "% python --version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Wkk9KK2YUfo"
      },
      "source": [
        "### 2. (3 pts) Isolate Inverse Document Frequency (IDF)\n",
        "Next up, we'll want to use our classifier efficiently and on smaller chunks of text than the _whole_ document. But here's the fine print:\n",
        "\n",
        "> Since we want to apply this in-line to tweet-sized chunks of text our term frequency will vary. However, this doesn't mean out IDF has to vary! So, if we pre-compute the `IDF` and store it as a vocabulary-sized vector, we'll be ablt to utilize it on the fly when we encounter text.\n",
        "\n",
        "In particular, complete the `make_IDF` function below that accepts the following arguments:\n",
        "- `documents`: a list of strings\n",
        "- `space ( = True)`: a boolean, optionally ignores space tokens from the stream\n",
        "\n",
        "and outputs the following objects:\n",
        "- `IDF`: a vector of inverse document frequencies in the same order as the `TDM` rows, _computed from base-$2$ logarithms_\n",
        "- `type_index`: the same `type_index` as the `make_TDM` function, which is pre-made in the starter code below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "RPUnN3prYUfp"
      },
      "outputs": [],
      "source": [
        "# A2:Function(3/3)\n",
        "\n",
        "def make_IDF(documents, space = True):\n",
        "    document_frequency = Counter()\n",
        "    for j, document in enumerate(documents):\n",
        "        frequency = Counter([t for s in sentokenize(document.lower(), space = space) \n",
        "                         for t in s])\n",
        "        document_frequency += Counter(frequency.keys())\n",
        "    type_index = {t:i for i, t in enumerate(sorted(list(document_frequency.keys())))}\n",
        "    document_frequency = np.array(list(document_frequency.values()))\n",
        "\n",
        "    #--- your code starts here\n",
        "    IDF = -np.log2(document_frequency/len(documents))\n",
        "    #--- your code stops here\n",
        "\n",
        "    return IDF, type_index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Tlyq6ZGyTKP"
      },
      "source": [
        "For reference, your output should be:\n",
        "```\n",
        "array([8.90689060e+00, 6.02378763e-03, 3.81942775e+00, 4.81942775e+00,\n",
        "       5.00000000e+00, 5.04650670e-02, 2.27200765e-02, 3.64010405e+00,\n",
        "       7.32192809e+00, 2.30933563e-01])\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "mdd4teDdxoap"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([8.90689060e+00, 6.02378763e-03, 3.81942775e+00, 4.81942775e+00,\n",
              "       5.00000000e+00, 5.04650670e-02, 2.27200765e-02, 3.64010405e+00,\n",
              "       7.32192809e+00, 2.30933563e-01])"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# A2:SanityCheck\n",
        "\n",
        "IDF, type_index = make_IDF([x['text'] for x in newstweet])\n",
        "IDF[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JL6ir85JYUfq"
      },
      "source": [
        "### 3. (5 pts) Decapitate the Logistic Classifier to Evaluate Chunks for Tweetfullness\n",
        "Here, your job is to complete the `cbow_chunk` function, which must accept:\n",
        "- `chunk`: a list of tokens to be CBOW'd with the classifer for tweetfullness\n",
        "- `IDF`: an inverse document frequency vector, i.e., obtained from __Part 2__\n",
        "- `CoM`: a semantic representation, i.e., produced from __Part 1__\n",
        "- `type_index`: the row locations of types across all documents\n",
        "- `classifier`: the `LogisticRegression().fit()` model produced from __Part 1__\n",
        "\n",
        "For the latter (`classifier`) argument, you must look up the following attributes in [the docs](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html):\n",
        "- `classifier.coeff_`: the model's linear-multiplicative weights, $\\vec{w}\\in\\mathbb{R}^d$\n",
        "- `classifier.intercept_`: the model's bias term, $b$\n",
        "\n",
        "Next, we'll discuss the math for implementation.\n",
        "\n",
        "#### CBOW, by `chunk`\n",
        "Supposing our `chunk` is the token stream $\\vec{t} = [t_1,\\cdots,t_n]$, let's define \n",
        "$$\n",
        "CoM_{\\vec{t}} = \\left[CoM_{t_1},\\cdots,CoM_{t_n}\\right]\n",
        "$$ \n",
        "to be the semantic matrix from $\\mathbb{R}^{d\\times n}$. \n",
        "As usual, the semantics can be weighted by the (vector of) $n$ token IDF values:\n",
        "$$\n",
        "IDF_{\\vec{t}} = \\frac{1}{n}\\left[IDF_{t_1},\\cdots,IDF_{t_n}\\right],\n",
        "$$\n",
        "i.e., aggregated via an inner product to produce a $d$-dimensional semantic-chunk representation:\n",
        "$$\n",
        "CDM_{\\vec{t}} = CoM_{\\vec{t}}\\cdot IDF_{\\vec{t}}\n",
        "$$\n",
        " which we can apply this through the linear model to produce an overall weighted score:\n",
        "$$\n",
        "\\hat{z}_{\\vec{t}} = \\vec{w}\\cdot CDM_{\\vec{t}} + b,\n",
        "$$\n",
        "\n",
        "Now, each prediction score must finally be supplied through the [_logistic function_](https://en.wikipedia.org/wiki/Logistic_regression), $\\sigma$, to be _activated_ into an interpretable _positive prediction probability_ for the chunk's tweetfullness:\n",
        "$$\n",
        "\\text{probability the chunk }\\vec{t}\\text{ contains a tweet} = \n",
        "\\hat{y}_{\\vec{t}} = \n",
        "\\sigma\\left(\\hat{z}_{\\vec{t}}\\right) = \n",
        "\\frac{1}{1 + e^{-\\hat{z}_{\\vec{t}}}}\n",
        "$$\n",
        "\n",
        "Your job is to apply this math to complete the below function and produce the positive prediction probability, $\\hat{y}_{\\vec{t}}$, as the function's output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "PG0OkJqMYUfq"
      },
      "outputs": [],
      "source": [
        "# A3:Function(5/5)\n",
        "\n",
        "def cbow_chunk(chunk, IDF, CoM, type_index, classifier):\n",
        "\n",
        "    #--- your code starts here\n",
        "    chunk_type_idx = [type_index[t] for t in chunk if t in type_index]\n",
        "    CoM_chunk = CoM[chunk_type_idx, :]\n",
        "    IDF_chunk = IDF[chunk_type_idx]\n",
        "    IDF_chunk = IDF_chunk * (1/len(chunk))\n",
        "\n",
        "    CDM_chunk = CoM_chunk.T @ np.array([IDF_chunk]).T\n",
        "\n",
        "    z_chunk = (classifier.coef_ @ CDM_chunk + classifier.intercept_).sum()\n",
        "    yhat_t = 1 / (1 + np.exp(-z_chunk))\n",
        "    #--- your code stops here\n",
        "\n",
        "    return yhat_t"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njDnIRuE6b3H"
      },
      "source": [
        "For reference, your output should be:\n",
        "```\n",
        "(array([0.97342538]), array([0.18833657]))\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "SdmqCvHf6Wnt"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.9740040632292059, 0.18692058890329136)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# A3:SanityCheck\n",
        "\n",
        "(cbow_chunk(tokenize('They later posted in a tweet:'.lower()), IDF, CoM_d, type_index, classifier),\n",
        " cbow_chunk(tokenize('Twitter is a social networking site.'.lower()), IDF, CoM_d, type_index, classifier))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5dBLsN9YUfq"
      },
      "source": [
        "### 4. (5) Evaluate Chunks From a Point in the Stream, Up to a Size\n",
        "Next, you'll be implementing a scanning algorithm to evaluate all chunks up to a size. Intuitively, since we know tweets can only occupy `280` characters, we need only check all `280`-character spans for tweetfullness. This is a little bit computationally intense so to ease up a bit we'll utilize our `sentokenizer`'s output to evaluate all sequences of consecutive `sentences` in a given document up to `chunk_size = 280` for tweetfullness.\n",
        "\n",
        "To modularize this process, you first task along these lines is to complete the `scan_chunks_from_point` function, which does:\n",
        "> scans sequences ($\\leq$ `chunk_size`) of a document's `sentences` for tweetfullness, starting from sentence-index `ix`.\n",
        "\n",
        "The arguments of this function are:\n",
        "- `ix`: sentence index from which to scan\n",
        "- `indices`: a list of sentence indices\n",
        "- `sentences`: a list of tokenized sentences from `sentokenize`\n",
        "- `IDF`: output from __Part 2__\n",
        "- `CoM`, `type_index`, `classifier`: output from __Part 1__\n",
        "- `chunk_size (= 280)`: the maximum chunk size to evaluate.\n",
        "\n",
        "The function must produce the following output:\n",
        "- `scores`: a list of sentence-sequence score outputs from the `cbow_chunk` function on the evaluated chunk\n",
        "- `spans`: a list of pairs `[ix,j]`, corresponding to the sentence indices for the first and last sentence in the evaluate chunk\n",
        "- `lengths`: the number of characters in the evaluated chunk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-C-AJANiYUfr"
      },
      "outputs": [],
      "source": [
        "# A4:Function(5/5)\n",
        "\n",
        "def scan_chunks_from_point(ix, indices, sentences, IDF, CoM, \n",
        "                           type_index, classifier, chunk_size = 280):\n",
        "     spans = []; scores = []; lengths = []\n",
        "\n",
        "     #--- your code starts here\n",
        "     start_sent = sentences[ix]\n",
        "     sent = start_sent\n",
        "     for i in indices:\n",
        "          if i != ix: \n",
        "               if len(''.join(sent + sentences[i])) <= chunk_size:\n",
        "                    sent += sentences[i]\n",
        "               else:\n",
        "                    continue\n",
        "          spans.append([ix, i])\n",
        "          lengths.append(len(''.join(sent)))\n",
        "          sent_score = cbow_chunk(sent, IDF, CoM, type_index, classifier)\n",
        "          scores.append(sent_score)\n",
        "          \n",
        "     #--- your code stops here\n",
        "\n",
        "     return scores, spans, lengths"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMzE99v7eQZX"
      },
      "source": [
        "For reference, your output should be:\n",
        "```\n",
        "([array([0.20171964]),\n",
        "  array([0.28072368]),\n",
        "  array([0.07079831]),\n",
        "  array([0.09715262])],\n",
        " [[0, 0], [0, 1], [0, 2], [0, 3]],\n",
        " [37, 77, 105, 150])\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Cx_qMcDHePPQ"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "([0.20034668037606637,\n",
              "  0.27963665665115317,\n",
              "  0.07445588995205407,\n",
              "  0.09989612516700039],\n",
              " [[0, 0], [0, 1], [0, 2], [0, 3]],\n",
              " [37, 77, 105, 150])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# A4:SanityCheck\n",
        "\n",
        "scan_chunks_from_point(0, [0, 1, 2, 3], \n",
        "                       sentokenize(''.join(['Twitter is a social networking site. ',\n",
        "                                            'The first tweet @jack ever posted was:\\n\\n',\n",
        "                                            '\"just setting up my twttr\"\\n\\n',\n",
        "                                            'which is now the first ever sold (for $2.9m).']).lower()), \n",
        "                       IDF, CoM_d, type_index, classifier, chunk_size = 280)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLzw5zW3YUfr"
      },
      "source": [
        "### 5. (4 pts) Evaluate All Chunks Up to a Size\n",
        "Next up, the task is to apply the `scan_chunks_from_point` function to each sentence-index in a given `document`. To get this going you'll have to:\n",
        "1. apply the `sentokenize` function to the _lowercased_ `document`, \n",
        "2. make a list of the sentence `indices`, and then\n",
        "3. make a list called `chunk_evaluations` that contains a sub-list for each index, `i` in `indices`.\n",
        "\n",
        "The only new function input is `document` which is an arbitrary string, and the function must return the `chunk_evaluations` list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "PVY0_nw7YUfs"
      },
      "outputs": [],
      "source": [
        "# A5:Function(4/4)\n",
        "\n",
        "def scan_all_chunks(document, IDF, CoM, type_index, classifier, chunk_size = 280):\n",
        "    \n",
        "    #--- your code starts here\n",
        "    sentoked = sentokenize(document.lower())\n",
        "    sentidc = list(range(len(sentoked)))\n",
        "    chunk_evaluations = []\n",
        "\n",
        "    for i in sentidc:\n",
        "        scores, spans, lengths = scan_chunks_from_point(i, sentidc[i:], sentoked, IDF, CoM, type_index, classifier, chunk_size = 280)\n",
        "        chunk_evaluations.append((scores, spans, lengths))\n",
        "    #--- your code stops here\n",
        "\n",
        "    return chunk_evaluations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7Mou7IFyog4"
      },
      "source": [
        "For reference, your output should be:\n",
        "```\n",
        "[([array([0.20171964]),\n",
        "   array([0.28072368]),\n",
        "   array([0.07079831]),\n",
        "   array([0.09715262])],\n",
        "  [[0, 0], [0, 1], [0, 2], [0, 3]],\n",
        "  [37, 77, 105, 150]),\n",
        " ([array([0.35241233]), array([0.04261771]), array([0.07997457])],\n",
        "  [[1, 1], [1, 2], [1, 3]],\n",
        "  [40, 68, 113]),\n",
        " ([array([0.00128139]), array([0.03443602])], [[2, 2], [2, 3]], [28, 73]),\n",
        " ([array([0.1681399])], [[3, 3]], [45])]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "eNzyV95pyopz"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[([0.20034668037606637,\n",
              "   0.27963665665115317,\n",
              "   0.07445588995205407,\n",
              "   0.09989612516700039],\n",
              "  [[0, 0], [0, 1], [0, 2], [0, 3]],\n",
              "  [37, 77, 105, 150]),\n",
              " ([0.3517337154674258, 0.046866069410801986, 0.08331761656499918],\n",
              "  [[1, 1], [1, 2], [1, 3]],\n",
              "  [40, 68, 113]),\n",
              " ([0.0021241108545449854, 0.03762236809753269], [[2, 2], [2, 3]], [28, 73]),\n",
              " ([0.1684347511136994], [[3, 3]], [45])]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# A5:SanityCheck\n",
        "\n",
        "scan_all_chunks(''.join(['Twitter is a social networking site. ',\n",
        "                         'The first tweet @jack ever posted was:\\n\\n',\n",
        "                         '\"just setting up my twttr\"\\n\\n',\n",
        "                         'which is now the first ever sold (for $2.9m).']).lower(), \n",
        "                IDF, CoM_d, type_index, classifier, chunk_size = 280)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1AKh-0_YUft"
      },
      "source": [
        "## 6. (8 pts) Get the best-scoring extract for a given document\n",
        "\n",
        "Here, your job is to take a `chunk_evaluations` output from the above `scan_all_chunks` function's application to a single document and get the single best extract.\n",
        "\n",
        "Here, we define 'best' from the above output by defining the following ranking score:\n",
        "\n",
        "> for each _prediction_ `chunk` $\\hat{t}$ from across _all_ sentence-starting points multiply its CBOW prediction score, $\\hat{y}_{\\hat{t}}$ (within its `scores`) by its chunk length, $\\ell\\left(\\hat{t}\\right)$ (in characters) out of $280$, and call this the `brevity_penalty` function, $\\beta\\left(\\hat{t}\\right)$. We'll call this the overall ranking score, $\\hat{Y}_{\\hat{t}}$:\n",
        "$$\n",
        "\\hat{Y}_{\\hat{t}} = \\hat{y}_{\\hat{t}}\\beta\\left(\\hat{t}\\right) = \\hat{y}_{\\hat{t}}\\frac{\\ell\\left(\\hat{t}\\right)}{280}\n",
        "$$\n",
        "\n",
        "In the below, your job is to complete the `get_extracts` function by determining the chunk, $\\hat{t}$, as a string called `character_extract`, which must correspond&mdash;via the span of sentence indices&mdash;to the largest value of $\\hat{Y}_{\\hat{t}}$ in the `chunk_evaluations` output. The only new argument is `idx` which corresponds to the document index within the `newstweet` object.\n",
        "\n",
        "Note: the function makes additional output that you should not modify, including the `docuement` (as a string) and a list of 10 random document (character) spans of the same size (number of characters) as your `character_extract`. This last,`rand_spans`, object is a simple baseline that we'll compare against in an evaluation, below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "4h6k4TLhYUft"
      },
      "outputs": [],
      "source": [
        "# A6:Function(8/8)\n",
        "\n",
        "import random as ra\n",
        "\n",
        "def get_extracts(newstweet, idx, IDF, CoM, type_index, classifier, chunk_size = 280):\n",
        "    brevity_penalty = lambda x: x/chunk_size\n",
        "    \n",
        "    document = newstweet[idx]['text']\n",
        "    chunk_evaluations = scan_all_chunks(document, IDF, CoM, type_index, classifier, \n",
        "                                        chunk_size = chunk_size)\n",
        "\n",
        "    #--- your code here\n",
        "    best_score = 0\n",
        "    best_spans = None\n",
        "    for chunk in chunk_evaluations:\n",
        "        for scores, spans, lengths in zip(*chunk):\n",
        "            penned_score = scores * brevity_penalty(lengths)\n",
        "            if penned_score > best_score: \n",
        "                best_score = penned_score\n",
        "                best_spans = spans\n",
        "\n",
        "    sentoked = sentokenize(document)\n",
        "    character_extract = ''.join(sum(sentoked[best_spans[0]: best_spans[1]], []))\n",
        "    #--- your code stops here\n",
        "    \n",
        "    ra.seed(691)\n",
        "    rand_spans = [[int(ra.random()*len(document)), 0] for _ in range(10)]\n",
        "    rand_spans = [[x[0],x[0]+len(character_extract)] for x in rand_spans]\n",
        "\n",
        "    return document, character_extract, rand_spans"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3EdHQ_T5ZA-"
      },
      "source": [
        "For reference, your ouput should be:\n",
        "```\n",
        "('Carrie Underwood kept to her signature shimmering style at the 2020 Country Music Association Awards on Wednesday night in Nashville.\\n\\nThe “Before He Cheats” singer hit the red carpet in a crystal-adorned, one-sleeved gown by Yousef Aljasmi. The ensemble was completely covered in sparkling stones and featured a high neck and a high slit. She paired the look with PVC sandals by Flor de Maria. Flor de Maria Rivera, a Peruvian shoe designer, officially launched her label in 2019. (Rivera’s label currently includes a range of styles, from strappy sandals and pumps to mules and a pair of knee-high boots.)\\n\\nCarrie Underwood on the red carpet at the 2020 CMA Awards. CREDIT: ABC\\n\\nUnderwood is one of the most fashionable country stars today. She first entered our lives in 2005 when she won singing competition show “American Idol.” Fifteen years later, her style has certainly evolved and she’s always one-to-watch on the red carpet, wearing Jimmy Choo, Giuseppe Zanotti and Rene Caovilla shoes with designer gowns.\\n\\nCarrie Underwood and husband Mike Fisher hit the red carpet at the 2020 CMAs in Nashville. CREDIT: ABC\\n\\nWatch on FN\\n\\nTonight, she was nominated for the Entertainer of the Year Award at the event. Underwood was up against Miranda Lambert, making the stars the first two female solo artists to be nominated in the CMA Awards category since 1979. Both lost to Eric Church.\\n\\nOther big winners tonight included Maren Morris — who took home the trophies for Female Vocalist of the Year, Single of the Year and Song of the Year — Luke Combs and Old Dominion.\\n\\nTo see more the CMA Awards red carpet arrivals, click through the gallery.',\n",
        " 'Carrie Underwood and husband Mike Fisher hit the red carpet at the 2020 CMAs in Nashville. CREDIT: ',\n",
        " [[1134, 1233],\n",
        "  [212, 311],\n",
        "  [727, 826],\n",
        "  [473, 572],\n",
        "  [1592, 1691],\n",
        "  [1076, 1175],\n",
        "  [1041, 1140],\n",
        "  [614, 713],\n",
        "  [985, 1084],\n",
        "  [1109, 1208]])\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "A5qs-dRF5ZJl"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('Carrie Underwood kept to her signature shimmering style at the 2020 Country Music Association Awards on Wednesday night in Nashville.\\n\\nThe “Before He Cheats” singer hit the red carpet in a crystal-adorned, one-sleeved gown by Yousef Aljasmi. The ensemble was completely covered in sparkling stones and featured a high neck and a high slit. She paired the look with PVC sandals by Flor de Maria. Flor de Maria Rivera, a Peruvian shoe designer, officially launched her label in 2019. (Rivera’s label currently includes a range of styles, from strappy sandals and pumps to mules and a pair of knee-high boots.)\\n\\nCarrie Underwood on the red carpet at the 2020 CMA Awards. CREDIT: ABC\\n\\nUnderwood is one of the most fashionable country stars today. She first entered our lives in 2005 when she won singing competition show “American Idol.” Fifteen years later, her style has certainly evolved and she’s always one-to-watch on the red carpet, wearing Jimmy Choo, Giuseppe Zanotti and Rene Caovilla shoes with designer gowns.\\n\\nCarrie Underwood and husband Mike Fisher hit the red carpet at the 2020 CMAs in Nashville. CREDIT: ABC\\n\\nWatch on FN\\n\\nTonight, she was nominated for the Entertainer of the Year Award at the event. Underwood was up against Miranda Lambert, making the stars the first two female solo artists to be nominated in the CMA Awards category since 1979. Both lost to Eric Church.\\n\\nOther big winners tonight included Maren Morris — who took home the trophies for Female Vocalist of the Year, Single of the Year and Song of the Year — Luke Combs and Old Dominion.\\n\\nTo see more the CMA Awards red carpet arrivals, click through the gallery.',\n",
              " 'Carrie Underwood and husband Mike Fisher hit the red carpet at the 2020 CMAs in Nashville. ',\n",
              " [[1134, 1225],\n",
              "  [212, 303],\n",
              "  [727, 818],\n",
              "  [473, 564],\n",
              "  [1592, 1683],\n",
              "  [1076, 1167],\n",
              "  [1041, 1132],\n",
              "  [614, 705],\n",
              "  [985, 1076],\n",
              "  [1109, 1200]])"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# A6:SanityCheck\n",
        "\n",
        "get_extracts(newstweet, 691, IDF, CoM_d, type_index, classifier, chunk_size = 280)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8s6NsJuYUft"
      },
      "source": [
        "### 7. (3 pts) Gather the holdout document set\n",
        "While the above output may/may not look as expected, we won't want to evaluate on just any documents for an estimate of performance. To produce a true evaluation of this tweet extractor we'll need to \n",
        "1. extract only from articles we expect (predict) to contain a tweet, and\n",
        "2. extract only from articles which didn't make it into the `classifier` training set, i.e., the _holdout set of documents_ from the `eval_tweet_prediction` function.\n",
        "\n",
        "So your job here is to complete the `holdout_data` function by constructing two `np.array()` as outpu\n",
        "- `idx_holdout`: a vector of the document indices from the `classifier`'s holdout set \n",
        "- `y_holdout`: a vector of $0$ and $1$ values, for which $1$ corresponds to articles from `idx_holdout` that contain a tweet, and $0$ for those that don't.\n",
        "\n",
        "Note: this requires applying \n",
        "```\n",
        "train_test_split(idx, y, test_size=0.33, random_state=state)\n",
        "```\n",
        "to a vector of $0$ and $1$ `y` values for the entire data set, in addition to a vector of indices (`idx`) for the entire data set.\n",
        "\n",
        "which are the indices and $0$ or $1$ 'has tweet' values for just those documents corresponding to the holdout set "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "gz_VEcYNYUfu",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# A7:Function(3/3)\n",
        "\n",
        "def holdout_data(newstweet, state = 0):\n",
        "\n",
        "    #--- your code starts here\n",
        "    idx = list(range(len(newstweet)))\n",
        "    y = [int(len(news['tweets']) > 0) for news in newstweet]\n",
        "    _, idx_holdout, _, y_holdout = train_test_split(idx, y, test_size=0.33, random_state=state)\n",
        "\n",
        "    #--- your code stops here\n",
        "    \n",
        "    return idx_holdout, y_holdout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAKLCXb8-pxf"
      },
      "source": [
        "For reference, your output should be:\n",
        "```\n",
        "(array([881, 406,  14, 708,  55, 786, 202, 567, 873, 299,  31, 657, 338,\n",
        "        350, 765,  77, 500, 255, 757, 746, 944, 215, 793, 503, 742, 239,\n",
        "        142, 141, 389, 605, 914, 810, 839, 530, 902, 691, 253, 883, 665,\n",
        "        935, 905, 332, 851, 456, 738, 474, 337, 670, 458, 613, 352, 304,\n",
        "        553, 251, 676, 959, 752, 683, 214, 316, 196, 933, 252, 794, 523,\n",
        "        899, 885, 884, 834, 453, 320,  65, 632, 577, 638, 888, 725, 580,\n",
        "         97, 443, 351, 828, 775,  60, 145, 465, 358, 452, 363, 424, 298,\n",
        "        279, 122, 261, 557, 415, 906, 693, 236,   8, 783, 101, 330, 597,\n",
        "        272, 175, 871, 626, 952, 717, 356,  34, 582, 306, 103, 345, 630,\n",
        "        732, 608, 743, 956,  27, 240, 447,  30, 826, 440, 264, 496, 432,\n",
        "        658, 830, 602, 310, 466, 247, 868, 954, 889, 413, 568, 342, 822,\n",
        "        416, 144, 736, 596, 813, 948, 249, 687, 747, 266, 478, 418, 681,\n",
        "          5, 471, 505, 692, 661, 319, 150, 856, 204, 311, 462, 825, 520,\n",
        "        230, 493, 644, 924, 113, 624,  37, 860, 518, 158, 154,  40, 529,\n",
        "        934, 821, 231, 833, 576, 494,  18, 840, 495, 784, 479, 451,  62,\n",
        "         79, 652, 193, 181, 294, 262, 200, 308, 951, 620, 295,   2, 436,\n",
        "        403, 724, 549,  85, 569,  75, 686, 380, 390, 362, 425, 270,   1,\n",
        "        939, 758, 801, 904, 695, 317, 721, 140, 267, 460, 271, 680, 533,\n",
        "        876, 312, 378, 283, 278,  50, 162, 625, 781, 722, 949, 958, 527,\n",
        "        769, 395, 489,  54,  45, 827, 172, 631, 578, 386, 482, 740,  39,\n",
        "        848, 156, 222, 210, 715, 920, 167, 864, 923, 409,  10, 105,  68,\n",
        "        841, 364, 942, 258, 710, 548, 811, 898, 886, 434,  20, 869, 603,\n",
        "        382, 285,  71,  49, 487, 366, 785, 116, 293,  76,  48, 588, 263,\n",
        "        776, 669, 762,  64, 766,  52, 402, 124, 622, 372, 831, 912, 435,\n",
        "        118,  12, 157, 127, 587]),\n",
        " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
        "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
        "        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
        "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
        "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
        "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
        "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "        0, 0, 0, 0, 0, 1, 0, 0, 0]))\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "hig7pWqA-qA-"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "([881,\n",
              "  406,\n",
              "  14,\n",
              "  708,\n",
              "  55,\n",
              "  786,\n",
              "  202,\n",
              "  567,\n",
              "  873,\n",
              "  299,\n",
              "  31,\n",
              "  657,\n",
              "  338,\n",
              "  350,\n",
              "  765,\n",
              "  77,\n",
              "  500,\n",
              "  255,\n",
              "  757,\n",
              "  746,\n",
              "  944,\n",
              "  215,\n",
              "  793,\n",
              "  503,\n",
              "  742,\n",
              "  239,\n",
              "  142,\n",
              "  141,\n",
              "  389,\n",
              "  605,\n",
              "  914,\n",
              "  810,\n",
              "  839,\n",
              "  530,\n",
              "  902,\n",
              "  691,\n",
              "  253,\n",
              "  883,\n",
              "  665,\n",
              "  935,\n",
              "  905,\n",
              "  332,\n",
              "  851,\n",
              "  456,\n",
              "  738,\n",
              "  474,\n",
              "  337,\n",
              "  670,\n",
              "  458,\n",
              "  613,\n",
              "  352,\n",
              "  304,\n",
              "  553,\n",
              "  251,\n",
              "  676,\n",
              "  959,\n",
              "  752,\n",
              "  683,\n",
              "  214,\n",
              "  316,\n",
              "  196,\n",
              "  933,\n",
              "  252,\n",
              "  794,\n",
              "  523,\n",
              "  899,\n",
              "  885,\n",
              "  884,\n",
              "  834,\n",
              "  453,\n",
              "  320,\n",
              "  65,\n",
              "  632,\n",
              "  577,\n",
              "  638,\n",
              "  888,\n",
              "  725,\n",
              "  580,\n",
              "  97,\n",
              "  443,\n",
              "  351,\n",
              "  828,\n",
              "  775,\n",
              "  60,\n",
              "  145,\n",
              "  465,\n",
              "  358,\n",
              "  452,\n",
              "  363,\n",
              "  424,\n",
              "  298,\n",
              "  279,\n",
              "  122,\n",
              "  261,\n",
              "  557,\n",
              "  415,\n",
              "  906,\n",
              "  693,\n",
              "  236,\n",
              "  8,\n",
              "  783,\n",
              "  101,\n",
              "  330,\n",
              "  597,\n",
              "  272,\n",
              "  175,\n",
              "  871,\n",
              "  626,\n",
              "  952,\n",
              "  717,\n",
              "  356,\n",
              "  34,\n",
              "  582,\n",
              "  306,\n",
              "  103,\n",
              "  345,\n",
              "  630,\n",
              "  732,\n",
              "  608,\n",
              "  743,\n",
              "  956,\n",
              "  27,\n",
              "  240,\n",
              "  447,\n",
              "  30,\n",
              "  826,\n",
              "  440,\n",
              "  264,\n",
              "  496,\n",
              "  432,\n",
              "  658,\n",
              "  830,\n",
              "  602,\n",
              "  310,\n",
              "  466,\n",
              "  247,\n",
              "  868,\n",
              "  954,\n",
              "  889,\n",
              "  413,\n",
              "  568,\n",
              "  342,\n",
              "  822,\n",
              "  416,\n",
              "  144,\n",
              "  736,\n",
              "  596,\n",
              "  813,\n",
              "  948,\n",
              "  249,\n",
              "  687,\n",
              "  747,\n",
              "  266,\n",
              "  478,\n",
              "  418,\n",
              "  681,\n",
              "  5,\n",
              "  471,\n",
              "  505,\n",
              "  692,\n",
              "  661,\n",
              "  319,\n",
              "  150,\n",
              "  856,\n",
              "  204,\n",
              "  311,\n",
              "  462,\n",
              "  825,\n",
              "  520,\n",
              "  230,\n",
              "  493,\n",
              "  644,\n",
              "  924,\n",
              "  113,\n",
              "  624,\n",
              "  37,\n",
              "  860,\n",
              "  518,\n",
              "  158,\n",
              "  154,\n",
              "  40,\n",
              "  529,\n",
              "  934,\n",
              "  821,\n",
              "  231,\n",
              "  833,\n",
              "  576,\n",
              "  494,\n",
              "  18,\n",
              "  840,\n",
              "  495,\n",
              "  784,\n",
              "  479,\n",
              "  451,\n",
              "  62,\n",
              "  79,\n",
              "  652,\n",
              "  193,\n",
              "  181,\n",
              "  294,\n",
              "  262,\n",
              "  200,\n",
              "  308,\n",
              "  951,\n",
              "  620,\n",
              "  295,\n",
              "  2,\n",
              "  436,\n",
              "  403,\n",
              "  724,\n",
              "  549,\n",
              "  85,\n",
              "  569,\n",
              "  75,\n",
              "  686,\n",
              "  380,\n",
              "  390,\n",
              "  362,\n",
              "  425,\n",
              "  270,\n",
              "  1,\n",
              "  939,\n",
              "  758,\n",
              "  801,\n",
              "  904,\n",
              "  695,\n",
              "  317,\n",
              "  721,\n",
              "  140,\n",
              "  267,\n",
              "  460,\n",
              "  271,\n",
              "  680,\n",
              "  533,\n",
              "  876,\n",
              "  312,\n",
              "  378,\n",
              "  283,\n",
              "  278,\n",
              "  50,\n",
              "  162,\n",
              "  625,\n",
              "  781,\n",
              "  722,\n",
              "  949,\n",
              "  958,\n",
              "  527,\n",
              "  769,\n",
              "  395,\n",
              "  489,\n",
              "  54,\n",
              "  45,\n",
              "  827,\n",
              "  172,\n",
              "  631,\n",
              "  578,\n",
              "  386,\n",
              "  482,\n",
              "  740,\n",
              "  39,\n",
              "  848,\n",
              "  156,\n",
              "  222,\n",
              "  210,\n",
              "  715,\n",
              "  920,\n",
              "  167,\n",
              "  864,\n",
              "  923,\n",
              "  409,\n",
              "  10,\n",
              "  105,\n",
              "  68,\n",
              "  841,\n",
              "  364,\n",
              "  942,\n",
              "  258,\n",
              "  710,\n",
              "  548,\n",
              "  811,\n",
              "  898,\n",
              "  886,\n",
              "  434,\n",
              "  20,\n",
              "  869,\n",
              "  603,\n",
              "  382,\n",
              "  285,\n",
              "  71,\n",
              "  49,\n",
              "  487,\n",
              "  366,\n",
              "  785,\n",
              "  116,\n",
              "  293,\n",
              "  76,\n",
              "  48,\n",
              "  588,\n",
              "  263,\n",
              "  776,\n",
              "  669,\n",
              "  762,\n",
              "  64,\n",
              "  766,\n",
              "  52,\n",
              "  402,\n",
              "  124,\n",
              "  622,\n",
              "  372,\n",
              "  831,\n",
              "  912,\n",
              "  435,\n",
              "  118,\n",
              "  12,\n",
              "  157,\n",
              "  127,\n",
              "  587],\n",
              " [0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0])"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# A7:SanityCheck\n",
        "\n",
        "holdout_data(newstweet, state = 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3IP5aP8YUfu"
      },
      "source": [
        "### 8. (3 pts) Build a ROUGE LCS evaluator for strings\n",
        "Here, your job will be to build a capacity for comparison of the system's top-scoring extracts to the true tweets which exist (as strings). In particular, this is a version of the [ROUGE metric](https://en.wikipedia.org/wiki/ROUGE_(metric)), particularly _ROUGE-L_.\n",
        "\n",
        "To compute ROUGE-L, your job is to complete the `ROUGE` function, which accepts two strings as arguments:\n",
        "- `str_y`: the target string, $\\vec{t}$, that we're hoping to extract\n",
        "- `str_yhat`: the predicted string, $\\hat{t}$ that was extracted\n",
        "We'll call the first the 'target' because we'll be viewing its tokens as being in need of being recovered, i.e., as 'positives', for which we'll compute ROUGE-L as a set positive prediction metrics, like precision, recall, and $F_1$.\n",
        "\n",
        "To calculate ROUGE-L, you must install the `pylcs` package:\n",
        "```\n",
        "% pip install pylcs\n",
        "```\n",
        "to compute the length of a _longest common subsequence_, `LCS` between the two strings. with the `LCS`, you must compute the following outputs:\n",
        "- `P`: ROUGE-L precision, as \n",
        "$$\n",
        "P_\\text{ROUGE-L} = \\frac{LCS}{\\ell\\left(\\hat{t}\\right)}\n",
        "$$\n",
        "- `R`: ROUGE-L recall, as \n",
        "$$\n",
        "R_\\text{ROUGE-L} = \\frac{LCS}{\\ell\\left(\\vec{t}\\right)}\n",
        "$$\n",
        "- `F1`: the ROUGE-L $F_1$, i.e., the 'metric' used for evaluating an extraction, overall, computed as the harmonic mean:\n",
        "$$\n",
        "F_{1,\\text{ROUGE-L}} = \n",
        "\\frac{2}{P_\\text{ROUGE-L}^{-1} + R_\\text{ROUGE-L}^{-1}}\n",
        "$$\n",
        "Note: each of these formulations can/will often need to be computed when its denominator is $0$. In these cases, the overall value of each component of ROUGE-L should be set to $0$s."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Wk8W-MgZD90R"
      },
      "outputs": [],
      "source": [
        "# % pip install pylcs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "4l-iRjozYUfu"
      },
      "outputs": [],
      "source": [
        "# A8:Function(3/3)\n",
        "\n",
        "import pylcs\n",
        "\n",
        "def ROUGE(str_y, str_yhat):\n",
        "    LCS = pylcs.lcs(str_yhat, str_y)\n",
        "\n",
        "    #--- your code starts here\n",
        "    P = 0\n",
        "    if len(str_yhat) > 0: \n",
        "        P = LCS / len(str_yhat)\n",
        "        \n",
        "    R = 0\n",
        "    if len(str_y) > 0: \n",
        "        R = LCS / len(str_y)\n",
        "\n",
        "    F1 = 0\n",
        "    if 1/P + 1/R != 0:\n",
        "        F1 = 2/(1/P + 1/R)\n",
        "\n",
        "    #--- your code stops here\n",
        "\n",
        "    return P, R, F1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rm5ZuZX4DeAT"
      },
      "source": [
        "For reference, your output should be:\n",
        "```\n",
        "(0.6875, 0.7333333333333333, 0.7096774193548386)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "2EkcaIwMDgAX"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.6875, 0.7333333333333333, 0.7096774193548386)"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# A8:SanityCheck\n",
        "\n",
        "ROUGE('said in a tweet', 'wrote in a tweet')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "absInE4NYUfu"
      },
      "source": [
        "### 9. (7 pts) Evaluate top article extract for each tweets in the holdout set\n",
        "\n",
        "Here, the goal is now to evaluate the performance of the extractor across all of the holdout articles. In particular, each holdout document must have its top-scoring `character_extract` and $10$ `rand_spans` baseline extractions `ROUGE`-evaluated against _all_ `true_tweet`s appearing in the article. \n",
        "\n",
        "In particular, your job is to complete the below function by:\n",
        "1. using its `span` attribute to extract the `true_tweet` from each `document` as a character slice;\n",
        "2. applying `ROUGE` to the pair:\n",
        "```\n",
        "P, R, F1 = ROUGE(true_tweet, character_extract);\n",
        "```\n",
        "3. updating the `top_pair = (F1, true_tweet, character_extract)` object (initialized to `(0,\"\",\"\")`) whenever the `character_extract`'s `F1` is larger than `top_pair[0]`;\n",
        "4. appending _all_ `R`, `P`, and `F1` values to their respective `Rs`, `Ps`, and `F1s` lists; and\n",
        "5. for each `rand_span` in `rand_spans`, computing the same ROUGE-L statistics via:\n",
        "```\n",
        "rP, rR, rF1 = ROUGE(true_tweet, rand_chunk)\n",
        "```\n",
        "  and subsequently _adding_ their values to the running numeric totals, `avgrP`, `avgrR`, and `avgrF1`.\n",
        "\n",
        "Note: the function's output is fixed as the record three objects, which can be understood as:\n",
        "- `top_pairs`: a list of top `(true_tweet, character_extract)` pairs for the holdout documents\n",
        "- `[avgPs, avgRs, avgF1s]`: a list of average-performance values, i.e., one for each document's `character_extract`\n",
        "- `[avgrPs, avgrRs, avgrF1s]`: a list of average average-performance values across _all_ of each `character_extract`'s `rand_spans`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "tcc8vGLjYUfu"
      },
      "outputs": [],
      "source": [
        "# A9:Function(6/6)\n",
        "\n",
        "def evaluate_top_extracts(newstweet, IDF, CoM, type_index, classifier, \n",
        "                          state = 0, chunk_size = 280):\n",
        "    idx_holdout, y_holdout = holdout_data(newstweet, state)\n",
        "    avgPs = []; avgRs = []; avgF1s = []; top_pairs = []\n",
        "    avgrPs = []; avgrRs = []; avgrF1s = []\n",
        "    for l, idx in enumerate(idx_holdout):\n",
        "        if not y_holdout[l]: \n",
        "            continue\n",
        "\n",
        "        document, character_extract, rand_spans = get_extracts(newstweet, idx, \n",
        "                                                               IDF, CoM, type_index, \n",
        "                                                               classifier, \n",
        "                                                               chunk_size = chunk_size)\n",
        "        num_rand_spans = len(rand_spans)\n",
        "        Ps = []; Rs = []; F1s = []; top_pair = (0,\"\",\"\")\n",
        "        for tweet, span in zip(newstweet[idx]['tweets'], newstweet[idx]['spans']):\n",
        "            avgrP, avgrR, avgrF1 = 0, 0, 0\n",
        "\n",
        "            #--- your code starts here\n",
        "            true_tweet = newstweet[idx]['text'][span[0]: span[1]]\n",
        "            P, R, F1 = ROUGE(true_tweet, character_extract)\n",
        "            Ps.append(P)\n",
        "            Rs.append(R)\n",
        "            F1s.append(F1)\n",
        "            if F1 > top_pair[0]:\n",
        "                top_pair = (F1, true_tweet, character_extract)\n",
        "\n",
        "            for rand_span in rand_spans:\n",
        "                rand_text = newstweet[idx]['text'][rand_span[0]: rand_span[1]]\n",
        "                P, R, F1 = ROUGE(rand_text, character_extract)\n",
        "                avgrP += P\n",
        "                avgrR += R\n",
        "                avgrF1 += F1\n",
        "\n",
        "            #--- your code stops here\n",
        "\n",
        "        top_pairs.append(top_pair)\n",
        "                \n",
        "        avgP = np.mean(Ps); avgR = np.mean(Rs); avgF1 = np.mean(F1)\n",
        "        avgRs.append(avgR); avgPs.append(avgP); avgF1s.append(avgF1)\n",
        "        avgrRs.append(avgrR/num_rand_spans) \n",
        "        avgrPs.append(avgrP/num_rand_spans) \n",
        "        avgrF1s.append(avgrF1/num_rand_spans)\n",
        "\n",
        "    return top_pairs, [avgPs, avgRs, avgF1s], [avgrPs, avgrRs, avgrF1s]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnjd46kKEFnm"
      },
      "source": [
        "For reference, your rank-1 output should be:\n",
        "```\n",
        "---\n",
        "Rank:  1 ; Score:  0.9963235294117647\n",
        "True tweet:  BREAKING: Oklahoma's horse-drawn #SoonerSchooner wagon tipped over, launching spirit squad members onto the field. Exploiting animals for sports is unnecessary & incredibly dangerous for animals AND humans. @OU_Football: KEEP HORSES OFF THE FIELD. https://t.co/SuVb3fH1xF\n",
        "Top extract:  BREAKING: Oklahoma's horse-drawn #SoonerSchooner wagon tipped over, launching spirit squad members onto the field. Exploiting animals for sports is unnecessary & incredibly dangerous for animals AND humans. @OU_Football: KEEP HORSES OFF THE FIELD. https://t.co/SuVb3fH1xF\n",
        "\n",
        "\n",
        "---\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "vQ6-xGjQEFz-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---\n",
            "Rank:  1 ; Score:  0.9556840077071291\n",
            "True tweet:  BREAKING: Oklahoma's horse-drawn #SoonerSchooner wagon tipped over, launching spirit squad members onto the field. Exploiting animals for sports is unnecessary & incredibly dangerous for animals AND humans. @OU_Football: KEEP HORSES OFF THE FIELD. https://t.co/SuVb3fH1xF\n",
            "Top extract:  BREAKING: Oklahoma's horse-drawn #SoonerSchooner wagon tipped over, launching spirit squad members onto the field. Exploiting animals for sports is unnecessary & incredibly dangerous for animals AND humans. @OU_Football: KEEP HORSES OFF THE FIELD. \n",
            "---\n",
            "---\n",
            "Rank:  2 ; Score:  0.7784256559766763\n",
            "True tweet:  this is the nina kapur i met in college: excellent at what she did while wearing a big smile on her face. i remember telling her i couldn't wait to watch her on national news one day... she deserved more. praying for her family and loved ones. https://t.co/EdzEW9r2NE\n",
            "Top extract:  this is the nina kapur i met in college: excellent at what she did while wearing a big smile on her face. i remember telling her i couldn't wait to watch her on national news one day... she deserved more. praying for her family and loved ones. https://t.co/EdzEW9r2NE\n",
            "\n",
            "\"I remember telling her i couldn't wait to watch her on national news one day… she deserved more. praying for her family and loved ones,\" she wrote.\n",
            "\n",
            "\n",
            "---\n",
            "---\n",
            "Rank:  3 ; Score:  0.7027027027027026\n",
            "True tweet:  \"I'm sorry, we wasted one of your years.\"   Watt to Watson after a tough season. https://t.co/mcTVK3ZeJG\n",
            "Top extract:  \"I'm sorry, we wasted one of your years.\"   Watt to Watson after a tough season. https://t.co/mcTVK3ZeJG\n",
            "\n",
            "NFL Films released postgame video from Sunday’s Week 17 loss to the Tennessee Titans. \n",
            "---\n"
          ]
        }
      ],
      "source": [
        "# A9:SanityCheck\n",
        "\n",
        "state = 0\n",
        "top_pairs, result, rand_result = evaluate_top_extracts(newstweet, IDF, CoM_d, type_index,\n",
        "                                                       classifier, state = state,\n",
        "                                                       chunk_size = 280)\n",
        "for ix, pair in enumerate(sorted(top_pairs, reverse = True)[:3]):\n",
        "    print(\"---\")\n",
        "    print(\"Rank: \", ix+1, \"; Score: \", pair[0])\n",
        "    print(\"True tweet: \", pair[1])\n",
        "    print(\"Top extract: \", pair[2])\n",
        "    print(\"---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0im7ueVehzB9"
      },
      "source": [
        "Finally, as one last system test for your code, the system's graphical evaluation output should be:\n",
        "![A9-expected-output](https://drive.google.com/uc?export=view&id=1SqcSOxGAqR0V98t-zjcQcg6hvJRXVm55)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "gi0unSMsYUfv"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# A9:SanityCheck\n",
        "\n",
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "fig = plt.figure(figsize = (6,6))\n",
        "bp = plt.boxplot(result, positions = [0.8, 1.8 , 2.8], patch_artist=True, \n",
        "                 medianprops = {'color': 'black', 'lw': 1},\n",
        "               boxprops = {'facecolor':'black', 'color':'black', 'alpha': 0.5})\n",
        "bprand = plt.boxplot(rand_result, positions = [1.2, 2.2 , 3.2], patch_artist=True, \n",
        "                     medianprops = {'color': 'black', 'lw': 1},\n",
        "                     boxprops = {'facecolor':'red', 'color':'red', 'alpha': 0.5})\n",
        "_ = plt.xticks([1, 2, 3], ['Precision', \"Recall\", \"$F_1$\"], fontsize = 15)\n",
        "_ = plt.yticks(fontsize = 15)\n",
        "_ = plt.legend([bp[\"boxes\"][0], bprand[\"boxes\"][0]], \n",
        "               [r\"$\\overline{F_1}: $\"+ str(round(100*np.median(result), 2)), \n",
        "                r\"$\\overline{F_1}: $\"+ str(round(100*np.median(rand_result), 2))], \n",
        "               loc='upper right', fontsize = 15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rD8jrMp6oCFC"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "A1-module-A.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
