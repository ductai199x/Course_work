{"cells":[{"cell_type":"markdown","metadata":{"id":"automatic-monday"},"source":["## Module submission header\n","### Submission preparation instructions \n","_Completion of this header is mandatory, subject to a 2-point deduction to the assignment._ Only add plain text in the designated areas, i.e., replacing the relevant 'NA's. You must fill out all group member Names and Drexel email addresses in the below markdown list, under header __Module submission group__. It is required to fill out descriptive notes pertaining to any tutoring support received in the completion of this submission under the __Additional submission comments__ section at the bottom of the header. If no tutoring support was received, leave NA in place. You may as well list other optional comments pertaining to the submission at bottom. _Any distruption of this header's formatting will make your group liable to the 2-point deduction._\n","\n","### Module submission group\n","- Group member 1\n","    - Name: Xi Chen\n","    - Email: xc98@drexel.edu\n","- Group member 2\n","    - Name: Tai Nguyen\n","    - Email: tdn47@drexel.edu\n","- Group member 3\n","    - Name: Tien Nguyen\n","    - Email: thn44@drexel.edu\n","- Group member 4\n","    - Name: Raymond Yung\n","    - Email: raymond.yung@drexel.edu\n","\n","### Additional submission comments\n","- Tutoring support received: NA\n","- Other (other): NA"]},{"cell_type":"markdown","metadata":{"id":"vulnerable-producer"},"source":["# DSCI 691: Natural language processing with deep learning <br> Assignment 5: Open Information Extraction (OIE)"]},{"cell_type":"markdown","metadata":{"id":"MXan1aXf_xqv"},"source":["## Overview\n","So what can RNN's do besides language modeling? Well, as sequence-based models, they're especially adept at sequential tasks, e.g., such as tagging. This includes NER, which we saw many variants of in __Chapter 2__, including that defined in the paper: [Question-Answer Driven Semantic Role Labeling: Using Natural Language to Annotate Natural Language](https://homes.cs.washington.edu/~lsz/papers/hlz-emnlp15.pdf)\n","\n","However, that task has since been updated quite a bit to accept/predict more generalized Q/A structures based on predicates as discussed in their follow up on [Crowdsourcing Question-Answer Meaning Representations](https://www.aclweb.org/anthology/N18-2089.pdf) (QAMRs), which produced--at scale--a noisy-but-informative set of generalized Q/A extending the QA-SRL task, whose code can be [found here](https://github.com/uwnlp/qamr). SOTA for this task was ultimately assumed by a more-modern (and computationally expensive!) transformer-based architecture (which we'll define in future chapters) but at the time (in 2018) biLSTMs were still in vogue (and still are quite high-performing) and the authors likewise defined an interesting on based on the concept of [Supervised Open Information Extraction](https://www.aclweb.org/anthology/N18-1081.pdf), and even made their (now depricated) code (based in tensorflow/keras) available\n","- https://github.com/gabrielStanovsky/supervised-oie/blob/master/src/rnn/model.py\n","\n","Once reviewing this paper, we'll see that highly-general Q/A systems can be trained via the BIO NER sequence labeling format by carefully pre-processing data and specifying a relatively-simple tag set."]},{"cell_type":"markdown","metadata":{"id":"VpzH6vUDY2SW"},"source":["### Review: Semantic Role Labeling (SRL)\n","Recall the Q/A _semantic role labeling_, where the goal was to identify token sequences&mdash;like named entities&mdash;that _answer_ specific, role-based questions about _entities_ contained within text, like who, what, when, where, and why? We discussed how this task clearly connects to NER by answering summary-level questions about sentences with specific points of justification for those answers:\n","```\n","|     UCD     |finished|the| 2006  |championship|as|Dublin|champions|,|\n","|B-who1/B-who2|   Q1   | 0 |B-what1|  I-what1   |0 |B-as1 |  I-as1  |0|\n"," ---------------------------------------------------------------------\n","|  by  | beating |       St       | Vincents  |in|  the  | final |.| \n","|B-how1|Q2/I-how1|    B-whom2     |  I-whom2  |0 |B-when2|I-when2|0|\n","```\n","\n","The excerpt was taken from [the QA-SRL dataset](https://huggingface.co/datasets/qa_srl) that we explored in __Chapter 2__ and is available from the `datasets` module and its [documentation is here](https://homes.cs.washington.edu/~lsz/papers/hlz-emnlp15.pdf). \n","\n"]},{"cell_type":"markdown","metadata":{"id":"z49Mx8rnJYN6"},"source":["### Updated Task: Open Information Extraction\n","Instead of QA-SRL, in this Assignment (worth __78 points__) we'll explore a continuation of that work to a more generalized _Open Information Extraction_ task and biLSTM model who's [documentation can be found here](https://www.aclweb.org/anthology/N18-1081.pdf), and [experimental data can be found in this repository here](https://github.com/gabrielStanovsky/supervised-oie).\n","\n","So long as we conform to the following `datasets` schema from our NER experiments:\n","```\n","DatasetDict({\n","    train: Dataset({\n","        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n","        num_rows: 14041\n","    })\n","    validation: Dataset({\n","        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n","        num_rows: 3250\n","    })\n","    test: Dataset({\n","        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n","        num_rows: 3453\n","    })\n","})\n","```\n"]},{"cell_type":"markdown","metadata":{"id":"37uQVSEXHNqk"},"source":["## Utilities"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"YP14x7hM_w5N"},"outputs":[],"source":["import json\n","import os\n","import re\n","\n","import torch\n","\n","import pandas as pd\n","\n","from tqdm.auto import tqdm"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"jW3xB5frHRH9"},"outputs":[],"source":["# Chapter 5 utility file\n","exec(open('./05-utilities.py').read())\n","# from utilities import *\n","\n","# Random seed\n","RAND_SEED = 691"]},{"cell_type":"markdown","metadata":{"id":"PDem-MRj8KJB"},"source":["Additionally, let's go ahead and load the pre-trained vectors we'll be using for this assignment from gensim:"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"sziyXnVUZ3wm"},"outputs":[],"source":["# %pip install gensim"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"3Ir-FNVH8Plf"},"outputs":[],"source":["import gensim.downloader as api\n","\n","word_dim = 50\n","model = api.load(f\"glove-wiki-gigaword-{word_dim}\")"]},{"cell_type":"markdown","metadata":{"id":"j4Oqe2oZ2egB"},"source":["## Experiment"]},{"cell_type":"markdown","metadata":{"id":"fMC_hFkK9J-9"},"source":["### 1. (5 pts) Data loading\n","\n","As is typical, our first goal will be to pre-process the raw data format provided for this task into something that we can use to setup key objects we'll need like a `Vocab` and `torch.utils.data.Dataset`. \n","We should be able to utilize our vocabulary loader (and other most code) from __Chapter 5__ to build out these objects. But first, we need to load the raw data into memory. \n","\n","Note: these data have no POS tags, and the paper we're following does! So we'll have to do a bit of featurization ahead.\n","\n","Your first task will be to familiarize yourself with the raw data format so that you may complete the definition of `load_data` below. Specifically, you will complete the extraction of a single sample within the nested loop that is iterating over data splits and data examples. You will want to populate the following key-value pairs in the allocated `sample` dictionary:\n","- `tokens`: The tokens in the sample\n","- `ner_tags`: The BIO tags which we will be predicting\n","- `p_ix`: The integer index of the predicate term\n","- `id`: The run ID\n","- `pos_tags`: A blank list (we'll featurize this a bit later...)\n","\n","Note: You should lowercase your tokens and NER labels."]},{"cell_type":"code","execution_count":5,"metadata":{"id":"KRU1nnDZULLa"},"outputs":[],"source":["# A1:Function(5/5)\n","\n","def load_data():\n","    ds = {}\n","    for fold in ['train', 'train.noisy', 'test', 'validation']:\n","        ds[fold] = []\n","\n","        # Read raw data as Pandas DataFrame\n","        df = pd.read_csv(f'./data/{fold}.oie.conll', sep='\\t', header=0, keep_default_na = False)\n","        \n","        # Split into samples\n","        df_samples = [df[df.run_id == run_id] for run_id in sorted(set(df.run_id.values))]\n","\n","        # Iterate over each sample\n","        for record in df_samples:\n","            sample = {}\n","            \n","            # --- Your code starts here ---\n","            sample['tokens'] = list(map(lambda s: s.lower(), record.word.tolist()))\n","            sample['ner_tags'] = list(map(lambda s: s.lower(), record.label.tolist()))\n","            sample['p_ix'] = record.head_pred_id.iloc[0]\n","            sample['id'] = record.run_id.iloc[0]\n","            sample['pos_tags'] = []\n","            # --- Your code ends here ---\n","\n","            if sample and len(sample['tokens']) and len(sample['tokens']) == len(sample['ner_tags']):\n","                ds[fold].append(sample)\n","          \n","        print(fold, \": \", len(ds[fold]), \" has sample instances\")\n","    \n","    # merge the noisy training set with the normal training set\n","    ds['train'] = ds['train'] + ds['train.noisy']\n","    del ds['train.noisy']\n","\n","    return ds"]},{"cell_type":"markdown","metadata":{"id":"CdwPnzYs_pnK"},"source":["As a sanity check, you should expect to see:\n","\n","```\n","rain :  5078  has sample instances\n","train.noisy :  18030  has sample instances\n","test :  1730  has sample instances\n","validation :  1673  has sample instances\n","{'id': 0, 'tokens': ['courtaulds', \"'\", 'spinoff', 'reflects', 'pressure', 'on', 'british', 'industry', 'to', 'boost', 'share', 'prices', 'beyond', 'the', 'reach', 'of', 'corporate', 'raiders', '.'], 'pos_tags': [], 'ner_tags': ['a0-b', 'a0-i', 'a0-i', 'p-b', 'a1-b', 'a1-i', 'a1-i', 'a1-i', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o'], 'p_ix': 3}\n","```"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"aOulRhEy_uta"},"outputs":[{"name":"stdout","output_type":"stream","text":["train :  5078  has sample instances\n","train.noisy :  18030  has sample instances\n","test :  1730  has sample instances\n","validation :  1673  has sample instances\n","{'tokens': ['courtaulds', \"'\", 'spinoff', 'reflects', 'pressure', 'on', 'british', 'industry', 'to', 'boost', 'share', 'prices', 'beyond', 'the', 'reach', 'of', 'corporate', 'raiders', '.'], 'ner_tags': ['a0-b', 'a0-i', 'a0-i', 'p-b', 'a1-b', 'a1-i', 'a1-i', 'a1-i', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o'], 'p_ix': 3, 'id': 0, 'pos_tags': []}\n"]}],"source":["# A1:SanityCheck\n","\n","ds = load_data()\n","print(ds['train'][0])"]},{"cell_type":"markdown","metadata":{"id":"jNE_T3Xuf240"},"source":["### 2. (3 pts) Vocabulary Construction\n","\n","Next, we should setup our `Vocab` for this task. Since the `Vocab` class (in our utility file) is nicely abstracted, we can reuse it here on the `train` fold of our data. \n","\n","Additionally, let's use the Vocab class to encode our target variable, i.e., the NER tag space (our classification classes). Since there shoudn't be any unknowns in the target class, we'll enforce this by training the vocab to cover all target class labels across all folds of the data set.\n","\n","Your task will be to complete the `init_vocab` function below, which is given the `ds` splits of our data and must return two vocabulary objects, `token_vocab` and `ner_vocab`. "]},{"cell_type":"code","execution_count":7,"metadata":{"id":"1jBdlLE7C2-U"},"outputs":[],"source":["# A2:Function(3/3)\n","\n","def init_vocab(ds):\n","    # Initialize Vocab objects for tokens and NER tags\n","    token_vocab = Vocab()\n","    ner_vocab = Vocab(stream = 'ner_tags', target = True)\n","\n","    # --- Your code starts here ---\n","    token_vocab.train(ds[\"train\"])\n","    ner_vocab.train(ds[\"train\"])\n","    # --- Your code ends here ---\n","\n","    return token_vocab, ner_vocab"]},{"cell_type":"markdown","metadata":{"id":"ZOyiG8CnDLDl"},"source":["As a sanity check, you should expect the following:\n","\n","```\n","Token vocab length: 17606\n","Encoded `the` token: 15\n","NER vocab length: 15\n","NER vocab: {'a0-b': 0, 'a0-i': 1, 'p-b': 2, 'a1-b': 3, 'a1-i': 4, 'o': 5, 'a2-b': 6, 'a2-i': 7, 'p-i': 8, 'a3-b': 9, 'a3-i': 10, 'a5-b': 11, 'a4-b': 12, 'a4-i': 13, 'a5-i': 14}\n","Encoded `p-b` NER label: 2\n","```"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"of6zNAJUDOmC"},"outputs":[{"name":"stdout","output_type":"stream","text":["Token vocab length: 17606\n","Encoded `the` token: 15\n","NER vocab length: 15\n","NER vocab: {'a0-b': 0, 'a0-i': 1, 'p-b': 2, 'a1-b': 3, 'a1-i': 4, 'o': 5, 'a2-b': 6, 'a2-i': 7, 'p-i': 8, 'a3-b': 9, 'a3-i': 10, 'a5-b': 11, 'a4-b': 12, 'a4-i': 13, 'a5-i': 14}\n","Encoded `p-b` NER label: 2\n"]}],"source":["# A2:SanityCheck\n","\n","token_vocab, ner_vocab = init_vocab(ds)\n","\n","print('Token vocab length:', len(token_vocab._word2idx))\n","print('Encoded `the` token:', token_vocab.encode('the'))\n","print('NER vocab length:', len(ner_vocab._word2idx))\n","print('NER vocab:', ner_vocab._word2idx)\n","print('Encoded `p-b` NER label:', ner_vocab.encode('p-b'))"]},{"cell_type":"markdown","metadata":{"id":"Sz4urV4H-A0g"},"source":["It looks like there could be up to 6 arguments and a predicate per sentence in this data set, so the number of target classes will have to include `'aX-b'` and `'aX-i'` (for `X in range(6)`). So, including these for each of the six and predicate (`'p-b'` and `'p-i'`), in additon to the null, `'o'` class, it appears the target space will result in a 15-dimensional system output (final layer) requirement."]},{"cell_type":"markdown","metadata":{"id":"ed5dPH_4fX67"},"source":["### 3. (8 pts) Pre-processing for POS features\n","Now that we've got our vocab set up, we're almost ready to construct our data loader, but there's unfortunately a catch&mdash;since the data doesn't have a POS (the authors leverage POS features) column, we'll need to implement a tagger to fill these data in. As a quick solution, we'll use nltk's POS tagging capabilities to set us up with some solid tags."]},{"cell_type":"code","execution_count":9,"metadata":{"id":"_QAeY9gZbqQV"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     C:\\Users\\Tai Nguyen\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n"]},{"data":{"text/plain":["True"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["# !pip install nltk\n","import nltk\n","nltk.download('averaged_perceptron_tagger')"]},{"cell_type":"markdown","metadata":{"id":"3N7cOV57EUee"},"source":["Your task will be to utilize nltk below to complete the definition of `add_pos_tags` by:\n","- For each `sample['tokens']`, extract [POS tags](https://www.nltk.org/api/nltk.tag.html), which should be stored in the sample dictionary under the key `pos_tags`\n","- Training a `pos_vocab` object on the `train` split of `ds`\n","\n","Hint: To get the tags of the tokens, use `nltk`'s `pos_tag` over the tokens of the sample!"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"1ltWdEKLb6vi"},"outputs":[],"source":["# A3:Function(8/8)\n","\n","from nltk import pos_tag\n","\n","def get_pos_tags(ds, retag=False):\n","    # Initialize Vocab object for POS tags\n","    pos_vocab = Vocab(stream = 'pos_tags')\n","\n","    # If available, re-load data\n","    if os.path.exists('./data/data.json') and not retag:\n","        ds = json.load(open('./data/data.json'))\n","    else:\n","\n","        for fold in ds:\n","            for sample in tqdm(ds[fold]):\n","                \n","                # --- Your code starts here ---\n","                sample[\"pos_tags\"] = [i.lower() for i in list(zip(*pos_tag(sample[\"tokens\"])))[1]]\n","                # --- Your code ends here ---\n","        \n","        with open('./data/data.json', 'w+') as f:\n","            f.write(json.dumps(ds))\n","\n","    # --- Your code starts here ---\n","    pos_vocab.train(ds[\"train\"])\n","    # --- Your code ends here --- \n","\n","    return ds, pos_vocab"]},{"cell_type":"markdown","metadata":{"id":"N6DUdN5ZIVbp"},"source":["As a sanity check, you should expect:\n","\n","```\n","100%|██████████| 23108/23108 [00:35<00:00, 659.25it/s]\n","100%|██████████| 1730/1730 [00:01<00:00, 875.83it/s]\n","100%|██████████| 1673/1673 [00:01<00:00, 885.10it/s]\n","\n","Train example: {'id': 0, 'tokens': ['courtaulds', \"'\", 'spinoff', 'reflects', 'pressure', 'on', 'british', 'industry', 'to', 'boost', 'share', 'prices', 'beyond', 'the', 'reach', 'of', 'corporate', 'raiders', '.'], 'pos_tags': ['NNS', 'POS', 'NN', 'VBZ', 'NN', 'IN', 'JJ', 'NN', 'TO', 'VB', 'NN', 'NNS', 'IN', 'DT', 'NN', 'IN', 'JJ', 'NNS', '.'], 'ner_tags': ['a0-b', 'a0-i', 'a0-i', 'p-b', 'a1-b', 'a1-i', 'a1-i', 'a1-i', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o'], 'p_ix': 3}\n","POS vocab length: 46\n","Encoded POS tag `nn`: 4\n","POS tags: ['__UNK__', '', 'nns', 'pos', 'nn', 'vbz', 'in', 'jj', 'to', 'vb', 'dt', '.', 'vbd', 'vbg', 'cd', 'md', 'rb', 'wrb', 'fw', 'prp$', 'cc', ',', 'vbp', 'vbn', ':', '$', '``', \"''\", 'prp', 'ex', 'wdt', 'rp', 'rbr', 'jjs', 'sym', 'jjr', '(', ')', 'wp', 'nnp', 'rbs', 'pdt', 'nnps', '#', 'uh', 'wp$']\n","```"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"Y03MndVcIU_S"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train example: {'id': 0, 'tokens': ['courtaulds', \"'\", 'spinoff', 'reflects', 'pressure', 'on', 'british', 'industry', 'to', 'boost', 'share', 'prices', 'beyond', 'the', 'reach', 'of', 'corporate', 'raiders', '.'], 'pos_tags': ['NNS', 'POS', 'NN', 'VBZ', 'NN', 'IN', 'JJ', 'NN', 'TO', 'VB', 'NN', 'NNS', 'IN', 'DT', 'NN', 'IN', 'JJ', 'NNS', '.'], 'ner_tags': ['a0-b', 'a0-i', 'a0-i', 'p-b', 'a1-b', 'a1-i', 'a1-i', 'a1-i', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o'], 'p_ix': 3}\n","POS vocab length: 46\n","Encoded POS tag `nn`: 4\n","POS tags: ['__UNK__', '', 'nns', 'pos', 'nn', 'vbz', 'in', 'jj', 'to', 'vb', 'dt', '.', 'vbd', 'vbg', 'cd', 'md', 'rb', 'wrb', 'fw', 'prp$', 'cc', ',', 'vbp', 'vbn', ':', '$', '``', \"''\", 'prp', 'ex', 'wdt', 'rp', 'rbr', 'jjs', 'sym', 'jjr', '(', ')', 'wp', 'nnp', 'rbs', 'pdt', 'nnps', '#', 'uh', 'wp$']\n"]}],"source":["# A3:SanityCheck\n","\n","retag = False  # Set to True to re-build if you make a change!\n","ds, pos_vocab = get_pos_tags(ds, retag=retag)\n","\n","print('Train example:', ds['train'][0])\n","print('POS vocab length:', len(pos_vocab._word2idx))\n","print('Encoded POS tag `nn`:', pos_vocab.encode('nn'))\n","print('POS tags:', list(pos_vocab._word2idx.keys()))"]},{"cell_type":"markdown","metadata":{"id":"FZbxVw3ULOBQ"},"source":["### 4. (11 pts) Forming a multi-modal vector representation\n","Following the authors' approach, we'll model an input instance via the triple: $(\\vec{t}, \\vec{s}, p)$, where $\\vec{t}$ is the input sentence's sequence of tokens, $\\vec{s}$ is the input sentence's sequence of part of speech (POS) tags, and $p$ is the word index of the predicate's syntactic head. Supposing we have embedding matrices for the token and POS vocabularies, $V_W$ and $V_S$, the model will extract a joint feature with the predicate and position in the text stream, $v(i,p)$, as:\n","$$\n","v(i, p) = \\left[v_{W}(t_i), v_{S}(s_i), v_{W}(t_p), v_{S}(s_p)\\right]\n","$$\n","where the brackets, $[\\cdot]$, indicate, again, a concatenation operation of all the contained vectors, into a single (flat) feature vector. Like the authors, we'll allow $5$-dimensional POS-embeddings and utilize the same-as-before 50-dimensional GloVe pre-trained vectors, placing each $v(i, p) \\in \\mathbb{R}^{110}$, i.e., making our input size a $110$-dimensional type-POS dense representation. Note: perhaps the most challenging aspect of this build (and __Chapter 6's__ RNN LM) is again the data loader's padding requirement, i.e., that _all_ sentences should be padded to the same length. This is handled in a pre-processing pass on the data.\n","\n","For this question, your task will be to complete the PyTorch Datset class `PyOIE` by:\n","- Completing its `__len__` definition\n","- Completing its `__getitem__` definition\n","- Implementing the `_featurize` function, which takes a sample and adds a new set of PyTorch tensors to the data containers\n","  - For this step, be sure to pad all input using `self.pad_length` as the desired sequence length. Additionally, use the null string `''` as the pad token and pad POS tag (both will map to 1, per our `Vocab` class). For NER tags (aka `label`), use the `o` tag for padding."]},{"cell_type":"code","execution_count":12,"metadata":{"id":"EYfRZ3CC-oNh"},"outputs":[],"source":["# A4:Class(11/11)\n","\n","class PyOIE(torch.utils.data.Dataset):\n","\n","    \"\"\"\n","    A PyTorch Dataset wrapper for the OIE dataset!\n","\n","    This wrapper is instantiated by providing:\n","    * ds_split: the specific split from OIE\n","    * vocab, pos_vocab, ner_vocab: the vocabs needed for OIE\n","    \"\"\"\n","\n","    def __init__(self, ds_split, vocab, pos_vocab, ner_vocab, pad_len=32):\n","        # cache parameters\n","        self._vocab = vocab\n","        self._pos_vocab = pos_vocab\n","        self._ner_vocab = ner_vocab\n","        self._raw = ds_split\n","\n","        # set up our data containers \n","        # note: we will be creating a list of tensors\n","        self._x_tok = []; self._x_pos = []\n","        self._p_tok = []; self._p_pos = []\n","        self._label = []\n","\n","        # setting pad to max sequence length\n","        max_len = max([len(r['tokens']) for r in self._raw]) \n","        self.pad_length = min(pad_len, max_len)\n","        \n","        # pre-process our samples into structured features\n","        for r in tqdm(self._raw):\n","            self._featurize(r)\n","\n","    def __len__(self):\n","        return len(self._raw)\n","\n","    def __getitem__(self, item):\n","        # when calling a specific example,\n","        # we return both the input and the \n","        # gold label!\n","        out = {}\n","\n","        # --- Your code starts here ---\n","        out[\"x_tok\"] = self._x_tok[item]\n","        out[\"x_pos\"] = self._x_pos[item]\n","        out[\"p_tok\"] = self._p_tok[item]\n","        out[\"p_pos\"] = self._p_pos[item]\n","        out[\"label\"] = self._label[item]\n","        # --- Your code ends here ---\n","\n","        return out\n","\n","    def _featurize(self, sample):\n","        # --- Your code starts here ---\n","        len_toks = len(sample[\"tokens\"])\n","        npads = self.pad_length - len_toks\n","        tokens = (sample[\"tokens\"] + [''] * (npads))[:self.pad_length]\n","        pos_tags = (sample[\"pos_tags\"] + [''] * (npads))[:self.pad_length]\n","        pos_tags = list(map(lambda s: s.lower(), pos_tags))\n","        ner_tags = (sample[\"ner_tags\"] + ['o'] * (npads))[:self.pad_length]\n","        ner_tags = list(map(lambda s: s.lower(), ner_tags))\n","\n","        p_tok = ([ sample[\"tokens\"][sample[\"p_ix\"]] ] * len_toks \n","                + [''] * (npads))[:self.pad_length]\n","        p_pos = ([ sample[\"pos_tags\"][sample[\"p_ix\"]].lower() ] * len_toks \n","                + [''] * (npads))[:self.pad_length]\n","\n","        enc_tokens = torch.LongTensor(list(map(self._vocab.encode, tokens)))\n","        enc_pos_tags = torch.LongTensor(list(map(self._pos_vocab.encode, pos_tags)))\n","        enc_ner_tags = torch.LongTensor(list(map(self._ner_vocab.encode, ner_tags)))\n","        enc_p_tok = torch.LongTensor(list(map(self._vocab.encode, p_tok)))\n","        enc_p_pos = torch.LongTensor(list(map(self._pos_vocab.encode, p_pos)))\n","\n","        self._x_tok.append(enc_tokens)\n","        self._x_pos.append(enc_pos_tags)\n","        self._p_tok.append(enc_p_tok)\n","        self._p_pos.append(enc_p_pos)\n","        self._label.append(enc_ner_tags)\n","        # --- Your code ends here ---\n","        \n","        # Note: This function does not return anything.\n","        # You should be appending pre-processed samples to the data containers\n","        # initalized in the __init__ funciton"]},{"cell_type":"markdown","metadata":{"id":"phikY6TDYrFG"},"source":["As a sanity check, you should expect:\n","\n","```\n","100%|██████████| 23108/23108 [00:01<00:00, 12107.84it/s]\n","100%|██████████| 1673/1673 [00:00<00:00, 11230.22it/s]\n","100%|██████████| 1730/1730 [00:00<00:00, 10895.23it/s]\n","\n","\n","x_tok tensor([ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,\n","        20,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1])\n","x_pos tensor([ 2,  3,  4,  5,  4,  6,  7,  4,  8,  9,  4,  2,  6, 10,  4,  6,  7,  2,\n","        11,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1])\n","p_tok tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1])\n","p_pos tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1])\n","label tensor([0, 1, 1, 2, 3, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n","        5, 5, 5, 5, 5, 5, 5, 5])\n","\n","```"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"dnJsdy9mYw9q"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"45e4ef1301ab43feb1063dc186965d3d","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/23108 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3556d471e80349f0a69cc63ebcdc62e9","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1673 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2bff35d1b2414f7e917111745c1f39c7","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1730 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n","x_tok tensor([ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,\n","        20,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1])\n","x_pos tensor([ 2,  3,  4,  5,  4,  6,  7,  4,  8,  9,  4,  2,  6, 10,  4,  6,  7,  2,\n","        11,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1])\n","p_tok tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1])\n","p_pos tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1])\n","label tensor([0, 1, 1, 2, 3, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n","        5, 5, 5, 5, 5, 5, 5, 5])\n"]}],"source":["# A4:SanityCheck \n","\n","train = PyOIE(ds['train'], token_vocab, pos_vocab, ner_vocab)\n","val = PyOIE(ds['validation'], token_vocab, pos_vocab, ner_vocab)\n","test = PyOIE(ds['test'], token_vocab, pos_vocab, ner_vocab)\n","\n","print()\n","for k, v in train[0].items():\n","  print(k, v)"]},{"cell_type":"markdown","metadata":{"id":"uK6v9xIog1bz"},"source":["### 5. (5 pts) Filling out the dense represenation\n","\n","Now that we've set up our vocabs, encoded our sequences, and created a full `PyOIE` dataset object, let's construct the initial representation matrix with a set of pre-trained GloVe vectors. \n","We'll randomly initialize normally-distributed random vectors for the tokens that are in our vocab that lack a pre-trained vector. \n","<!-- As we will unfreeze this embedding layer, this shouldn't be a problem since we'll backpropagate and update the embedding to learn reasonable vectors for these randomly initialized ones. -->"]},{"cell_type":"markdown","metadata":{"id":"XTupKmD3bLPS"},"source":["Now, your task is to finish the `get_embeddings` function per our description. Specifically, you should:\n","- Load the pre-trained vector, if it's in the model\n","- Else, randomly initialize one using a standard normal distribution"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"mXzACUe9g1n7"},"outputs":[],"source":["# A5:Function(5/5)\n","\n","def get_embeddings(vocab, model, vector_dim=300):\n","    torch.manual_seed(RAND_SEED)\n","\n","    vocab_size = len(vocab._word2idx)\n","\n","    wvs = torch.zeros((vocab_size, vector_dim))\n","\n","    # --- Your code starts here ---\n","    for k in vocab._word2idx:\n","        try:\n","            pretrained_idx = model.get_index(k)\n","            wvs[vocab._word2idx[k]] = torch.tensor(model[pretrained_idx])\n","        except KeyError:\n","            wvs[vocab._word2idx[k]] = torch.randn((50, ))    \n","    # --- Your code ends here ---\n","\n","    return wvs"]},{"cell_type":"markdown","metadata":{"id":"TmYIuGSHcK0n"},"source":["As a sanity check, you should expect:\n","\n","```\n","Word embedding shape: torch.Size([17606, 50])\n","Representation for `the`:\n","\t tensor([ 4.1800e-01,  2.4968e-01, -4.1242e-01,  1.2170e-01,  3.4527e-01,\n","        -4.4457e-02, -4.9688e-01, -1.7862e-01, -6.6023e-04, -6.5660e-01,\n","         2.7843e-01, -1.4767e-01, -5.5677e-01,  1.4658e-01, -9.5095e-03,\n","         1.1658e-02,  1.0204e-01, -1.2792e-01, -8.4430e-01, -1.2181e-01,\n","        -1.6801e-02, -3.3279e-01, -1.5520e-01, -2.3131e-01, -1.9181e-01,\n","        -1.8823e+00, -7.6746e-01,  9.9051e-02, -4.2125e-01, -1.9526e-01,\n","         4.0071e+00, -1.8594e-01, -5.2287e-01, -3.1681e-01,  5.9213e-04,\n","         7.4449e-03,  1.7778e-01, -1.5897e-01,  1.2041e-02, -5.4223e-02,\n","        -2.9871e-01, -1.5749e-01, -3.4758e-01, -4.5637e-02, -4.4251e-01,\n","         1.8785e-01,  2.7849e-03, -1.8411e-01, -1.1514e-01, -7.8581e-01])\n","Representation for `euromarket`:\n","\t tensor([ 0.8074, -0.6810,  1.0453, -0.4567,  0.7270, -0.5824, -0.2911,  0.3334,\n","         0.4072, -1.6747, -0.8621, -0.6207, -2.5802,  1.2279, -0.0549,  0.5757,\n","        -0.4442,  0.0988,  0.0253,  0.2364, -1.5998, -0.8065,  1.4552,  1.0078,\n","         1.0012,  0.2039, -0.6355,  0.9761, -2.5255,  2.1219, -0.0597, -0.3632,\n","        -2.4620,  1.8938, -0.8431, -0.4609, -0.4363, -0.7455,  3.3058, -0.1539,\n","         1.0984, -0.4961,  0.2065, -1.6130, -0.5355,  0.5481,  0.1270,  0.9623,\n","         0.6976,  0.8415])\n","```"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"A8jSWYGCcKRa"},"outputs":[{"name":"stdout","output_type":"stream","text":["Word embedding shape: torch.Size([17606, 50])\n","Representation for `the`:\n","\t tensor([ 4.1800e-01,  2.4968e-01, -4.1242e-01,  1.2170e-01,  3.4527e-01,\n","        -4.4457e-02, -4.9688e-01, -1.7862e-01, -6.6023e-04, -6.5660e-01,\n","         2.7843e-01, -1.4767e-01, -5.5677e-01,  1.4658e-01, -9.5095e-03,\n","         1.1658e-02,  1.0204e-01, -1.2792e-01, -8.4430e-01, -1.2181e-01,\n","        -1.6801e-02, -3.3279e-01, -1.5520e-01, -2.3131e-01, -1.9181e-01,\n","        -1.8823e+00, -7.6746e-01,  9.9051e-02, -4.2125e-01, -1.9526e-01,\n","         4.0071e+00, -1.8594e-01, -5.2287e-01, -3.1681e-01,  5.9213e-04,\n","         7.4449e-03,  1.7778e-01, -1.5897e-01,  1.2041e-02, -5.4223e-02,\n","        -2.9871e-01, -1.5749e-01, -3.4758e-01, -4.5637e-02, -4.4251e-01,\n","         1.8785e-01,  2.7849e-03, -1.8411e-01, -1.1514e-01, -7.8581e-01])\n","Representation for `euromarket`:\n","\t tensor([-0.9060, -1.0927, -1.3634,  0.8262, -0.2185, -0.2157,  1.8471,  0.2207,\n","        -0.4699, -0.3421,  1.4126,  0.5451,  1.2909,  2.2761,  2.3379, -0.8562,\n","        -0.9291,  0.3758,  0.3393, -0.2934, -0.3482,  0.6810,  0.4140,  0.6565,\n","        -0.2993, -0.7726,  0.4552,  1.2992,  0.5843, -0.2510,  0.5475, -0.8244,\n","        -0.8811, -0.7064, -1.2508, -2.1082,  1.7128,  0.8204, -1.6499, -1.4276,\n","        -1.5497, -0.7067, -0.0747, -0.6754,  1.3968,  0.1145,  1.6584, -1.4881,\n","         2.6331,  0.3039])\n"]}],"source":["# A5:SanityCheck\n","\n","wvs = get_embeddings(token_vocab, model, vector_dim=word_dim)\n","\n","print('Word embedding shape:', wvs.shape)\n","print('Representation for `the`:\\n\\t', wvs[token_vocab.encode('the')])\n","print('Representation for `euromarket`:\\n\\t', wvs[token_vocab.encode('euromarket')])"]},{"cell_type":"markdown","metadata":{"id":"4hWncYyYinUz"},"source":["### 6. (15 pts) Defining the neural LSTM tagger\n","\n","Now that we've loaded our initial token embeddings, we can define our model.\n","Specifically, we'll setup a biLSTM (like was proposed in the paper) that has the following layers:\n","- A token embedding, initialized from our `word_vectors` \n","  * Set `freeze=True`\n","  * Also, set `padding_idx=1`. If done, this tells PyTorch that an encoded 1 represents a pad token and PyTorch will not compute gradients for these tokens (thus saving us computation time!)\n","- A token embedding dropout layer that drops out a word representation with probability `word_dropout`\n","- A POS embedding, ranomly initialized (i.e., initialized normally, not with any pre-trained representations)\n","  * Also set `padding_idx=1` here as well!\n","- An LSTM layer\n","  * For this, we recommend setting `batch_first=True` so that sequences are expected to be of shape `(batch_size, sequence_length, d)` instead of the default expectation of `(sequence_length, batch_size, d)`\n","- A ReLU non-linearity\n","- A prediction dropout that drops out (prior to the last layer) with probability `pred_dropout`\n","- A prediction layer that maps from the biLSTM's output to the target, NER class space\n","  * For this layer, set `bias=False`\n","\n","After setting these layers up in `__init__`, you should complete the model definition by writing the `forward` function."]},{"cell_type":"code","execution_count":16,"metadata":{"id":"k4_7go1ViUlr"},"outputs":[],"source":["# A6:Class(15/15)\n","\n","class biLSTM(torch.nn.Module):\n","\n","    def __init__(self, \n","                 in_dim, hidden_dim, out_dim, \n","                 lstm_layers, word_vectors, \n","                 pos_vocab_size, pos_dim=5,\n","                 word_dropout=0.1,\n","                 pred_dropout=0.1,\n","                 bidirectional = True):\n","        # remember to start __init__ with a call to super!\n","        super(biLSTM, self).__init__()\n","\n","        torch.manual_seed(RAND_SEED)\n","\n","        self.hidden_dim = hidden_dim\n","        self.in_dim = in_dim\n","        self.out_dim = out_dim\n","        self.lstm_layers = lstm_layers\n","        self.num_directions = 2 if bidirectional else 1\n","\n","        # --- Your code starts here\n","        self.token_embed = torch.nn.Embedding.from_pretrained(word_vectors, padding_idx=1)\n","        self.pos_embed = torch.nn.Embedding(pos_vocab_size, pos_dim)\n","\n","        self.token_embed_dropout = torch.nn.Dropout(word_dropout)\n","        self.predict_dropout = torch.nn.Dropout(pred_dropout)\n","\n","        self.lstm = torch.nn.LSTM(\n","            in_dim, \n","            hidden_dim, \n","            lstm_layers, \n","            batch_first=True, \n","            bidirectional=bidirectional,\n","        )\n","\n","        self.relu = torch.nn.ReLU()\n","        self.predict_layer = torch.nn.Linear(hidden_dim * 2, out_dim, bias=False)\n","        # --- Your code ends here ---\n","\n","    def forward(self, x_tok, x_pos, p_tok, p_pos): \n","\n","        # --- Your code starts here ---\n","        tok_emb = self.token_embed(x_tok)\n","        tok_emb = self.token_embed_dropout(tok_emb)\n","        predicate_tok_emb = self.token_embed(p_tok)\n","        pos_emb = self.pos_embed(x_pos)\n","        pos_emb = self.token_embed_dropout(pos_emb)\n","        predicate_pos_emb = self.pos_embed(p_pos)\n","\n","        emb = torch.concat([tok_emb, pos_emb, predicate_tok_emb, predicate_pos_emb], dim=2)\n","        emb, _ = self.lstm(emb)\n","        emb = self.relu(emb)\n","        emb = self.predict_dropout(emb)\n","        z_hat = self.predict_layer(emb)\n","        # --- Your code ends here ---\n","\n","        return z_hat"]},{"cell_type":"markdown","metadata":{"id":"m1jNUlHViW4B"},"source":["As a sanity check, you should expect the following:\n","\n","```\n","biLSTM(\n","  (_embed): Embedding(17606, 50, padding_idx=1)\n","  (_pos_embed): Embedding(50, 5, padding_idx=1)\n","  (_embed_dropout): Dropout(p=0.1, inplace=False)\n","  (_lstm): LSTM(110, 256, num_layers=3, batch_first=True, bidirectional=True)\n","  (_ReLU): ReLU()\n","  (_pred_dropout): Dropout(p=0.1, inplace=False)\n","  (_pred): Linear(in_features=512, out_features=15, bias=False)\n",")\n","Number of parameters: 4795814\n","Number of (trainable) parameters: 3915514\n","Test Prediction shape: torch.Size([1, 32, 15])\n","```"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"fyNU3ejsiWhA"},"outputs":[{"name":"stdout","output_type":"stream","text":["biLSTM(\n","  (token_embed): Embedding(17606, 50, padding_idx=1)\n","  (pos_embed): Embedding(46, 5)\n","  (token_embed_dropout): Dropout(p=0.1, inplace=False)\n","  (predict_dropout): Dropout(p=0.1, inplace=False)\n","  (lstm): LSTM(110, 256, num_layers=3, batch_first=True, bidirectional=True)\n","  (relu): ReLU()\n","  (predict_layer): Linear(in_features=512, out_features=15, bias=False)\n",")\n","Number of parameters: 4795794\n","Number of (trainable) parameters: 3915494\n","Test Prediction shape: torch.Size([1, 32, 15])\n"]}],"source":["# A6:SanityCheck\n","\n","pos_dim = 5\n","pos_vocab_size = len(pos_vocab._word2idx)\n","\n","in_dim = 2 * word_dim + 2 * pos_dim\n","\n","hidden_dim = 256\n","\n","lstm_layers = 3\n","out_dim = len(ner_vocab._word2idx)\n","\n","biLSTM_net = to_gpu(biLSTM(in_dim, hidden_dim, out_dim, lstm_layers, wvs, pos_vocab_size, pos_dim=pos_dim))\n","\n","print(biLSTM_net)\n","print('Number of parameters:', sum(p.numel() for p in biLSTM_net.parameters()))\n","print('Number of (trainable) parameters:', sum(p.numel() for p in biLSTM_net.parameters() if p.requires_grad))\n","\n","# Testing that input properly flows through the model:\n","x = train[0]\n","x = {k: to_gpu(v.unsqueeze(0)) for k, v in x.items()}  # create batch size of 1\n","out = biLSTM_net(x['x_tok'], x['x_pos'], x['p_tok'], x['p_pos'])\n","print('Test Prediction shape:', out.shape)"]},{"cell_type":"markdown","metadata":{"id":"sUosNQ99sdmw"},"source":["### 7. (15 pts) Training the neural tagger\n","\n","Your next task will be to complete the definition of the `train` function below. \n","In this train function, you will need to:\n","* Configure an optimizer\n","  * We'll use Adagrad and will set the learning rate equal to `lr`\n","* Configure a loss function\n","  * This should be `CrossEntropyLoss` with `reduction='sum'`\n","* Configure two DataLoaders, one for the `train` data and one for the `val` data\n","  * Be sure to enable shuffling and set the proper batch size!\n","\n","Next, we'll want to configure a training loop that uses early stopping. \n","Specifically, `train_model` receives input parameters `max_epochs` and `patience`. `max_epochs` tells us the _maximum_ number of epochs we'll let our model train for and `patience` tells us how many epochs we are willing to wait where our model does __not__ improve on the validation set before exiting. \n","Within this loop, you should complete the two missing code chunks for the `train` and `val` iterations, respectively. Be sure to enable `.train()` and `.eval()` when appropriate.\n","\n","Note: We could use gradient clipping here, but since our model is not super complicated nor that deep (in time and depth), we will opt to not clip our gradients. \n"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"va8nuYUtlfSt"},"outputs":[],"source":["# A7:Function(15/15)\n","\n","def train_model(model: torch.nn.Module, train_ds, val_ds, lr=1e-2, batch_size=50, max_epochs=100, patience=5, gpu=True):\n","    torch.manual_seed(RAND_SEED)\n","    train_dl = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True, pin_memory=True)\n","    val_dl = torch.utils.data.DataLoader(val_ds, batch_size=128, pin_memory=True)\n","\n","    # --- Your code starts here ---\n","    loss_fn = torch.nn.CrossEntropyLoss(reduction=\"sum\")\n","    model = model.cuda()\n","    opt = torch.optim.Adagrad(model.parameters(), lr=lr)\n","    # --- Your code ends here --- \n","\n","    epoch = 0\n","    no_imp = 0\n","    best_val_loss = None \n","    while epoch < max_epochs and no_imp < patience:\n","        # print('=' * 50)\n","        # print('Begin epoch', epoch + 1)\n","        # print('-' * 50)\n","        \n","        train_loss = 0\n","        # --- Your code starts here ---\n","        train_pbar = tqdm(total=len(train_dl))\n","        train_pbar.set_description_str(f\"Epoch {epoch}: \")\n","        for i, batch in enumerate(train_dl):\n","            batch = {k: to_gpu(v) for k, v in batch.items()}\n","            opt.zero_grad()\n","            y_pred = model(batch['x_tok'], batch['x_pos'], batch['p_tok'], batch['p_pos'])\n","            y_pred = y_pred.flatten(0, 1)\n","            y_true = batch['label'].flatten()\n","            loss = loss_fn(y_pred, y_true)\n","            loss.backward()\n","            opt.step()\n","\n","            train_loss += loss.cpu().detach().item()\n","            avg_train_loss = train_loss / ((i + 1) * len(y_true))\n","\n","            train_pbar.set_postfix({\"train_loss\": avg_train_loss})\n","            train_pbar.update()\n","        # --- Your code ends here ---\n","\n","        val_loss = 0\n","        # --- Your code starts here ---\n","        with torch.no_grad():\n","            val_pbar = tqdm(total=len(val_dl))\n","            val_pbar.set_description_str(f\"Validating {epoch}: \")\n","            for i, batch in enumerate(val_dl):\n","                batch = {k: to_gpu(v) for k, v in batch.items()}\n","                y_pred = model(batch['x_tok'], batch['x_pos'], batch['p_tok'], batch['p_pos'])\n","                y_pred = y_pred.flatten(0, 1)\n","                y_true = batch['label'].flatten()\n","                loss = loss_fn(y_pred, y_true)\n","\n","                val_loss += loss.cpu().detach().item()\n","                avg_val_loss = val_loss / ((i + 1) * len(y_true))\n","\n","                val_pbar.set_postfix({\"val_loss\": avg_val_loss})\n","                val_pbar.update()\n","        # --- Your code ends here ---\n","\n","        if best_val_loss is None or val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            no_imp = 0\n","            torch.save(model, './data/PyOIE_bilstm.pt')\n","        else:\n","            no_imp += 1\n","\n","        print(f'Best validation loss: {best_val_loss:.2f} (Epochs without improvement: {no_imp})')\n","\n","        epoch += 1\n","        # print('=' * 50)\n","\n","    # Reload and return the best model\n","    return torch.load('./data/PyOIE_bilstm.pt')"]},{"cell_type":"markdown","metadata":{"id":"3KQ1wlyyqFVX"},"source":["As a sanity check, you should anticipate that your model can achieve a validation loss below `70_000`."]},{"cell_type":"code","execution_count":19,"metadata":{"id":"PSgnllGSqZay"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c351ed0b007946739010c4b0f047dd91","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/723 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ef32de52dc9f4686bccafbdb9d3937a5","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/14 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Best validation loss: 54181.60 (Epochs without improvement: 0)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"61c31e12c3004575ad5c2cf8ac4b5033","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/723 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3d65f8b3c3f241158fb2acfc32181c7d","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/14 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Best validation loss: 49163.67 (Epochs without improvement: 0)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4c30dc1b9cbb41fa8edb3fe98674754c","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/723 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"345be7a4e2ad41e1985667be0644e061","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/14 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Best validation loss: 47834.65 (Epochs without improvement: 0)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"afac282a95234b20be713fa6ccb94417","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/723 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a6df3448a779424dabbd4424db33601f","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/14 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Best validation loss: 47834.65 (Epochs without improvement: 1)\n"]}],"source":["# A7:SanityCheck\n","\n","lr = 1e-2\n","batch_size = 32\n","biLSTM_net = biLSTM(in_dim, hidden_dim, out_dim, lstm_layers, wvs, pos_vocab_size, pos_dim=pos_dim)\n","\n","best_bilstm = train_model(biLSTM_net, train, val, lr=lr, batch_size=batch_size, patience=1)"]},{"cell_type":"markdown","metadata":{"id":"o6JNM9-Ihy6v"},"source":["### 8. (8 pts) Evaluating the neural tagger\n","\n","Now that we've trained up a model, let's evaluate it on each split of the data. \n","Specifically, you will complete the `eval_tagger` function which will take a split of the data and will return the sklearn `classification_report` for that split."]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([ 3, 13, 14,  0, 10,  2,  0, 12,  0,  1, 13, 10, 12, 14, 12,  9,  5,  0,\n","        10, 14, 10,  9,  9, 12,  9,  7,  9,  6,  5, 13,  6,  1])"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["torch.argmax(torch.randn((32, 15)), dim=1)"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"XvornxO7tHXg"},"outputs":[],"source":["# A8:Function(8/8)\n","\n","from sklearn.metrics import classification_report\n","\n","def eval_tagger(model, data, batch_size=50, label_names=None):\n","    batch_loader = torch.utils.data.DataLoader(data, batch_size=batch_size)\n","\n","    model = model.eval()\n","\n","    preds = []; golds = []\n","    with torch.no_grad():\n","        val_pbar = tqdm(total=len(batch_loader))\n","        val_pbar.set_description_str(f\"Testing: \")\n","        val_loss = 0\n","        for i, batch in enumerate(batch_loader):\n","            batch = {k: to_gpu(v) for k, v in batch.items()}\n","            y_pred = model(batch['x_tok'], batch['x_pos'], batch['p_tok'], batch['p_pos'])\n","            y_pred = y_pred.flatten(0, 1)\n","            y_pred = torch.argmax(y_pred, dim=1)\n","            y_true = batch['label'].flatten()\n","\n","            preds += y_pred.detach().cpu().tolist()\n","            golds += y_true.detach().cpu().tolist()\n","\n","            val_pbar.update()\n","        # --- Your code ends here ---\n","\n","    return classification_report(golds, preds, target_names=label_names)"]},{"cell_type":"markdown","metadata":{"id":"0t2K7f5rdw2R"},"source":["As a sanity check, your model should be able to attain a training weighted average F1-score above 0.70 and a validation weighted average F1-score above 50."]},{"cell_type":"code","execution_count":22,"metadata":{"id":"1vf8p2Y8QPB-"},"outputs":[{"name":"stdout","output_type":"stream","text":["Evaluating train\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"af81122d6c9c427e83ffee65dcb26594","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/463 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["f:\\1-workdir\\pyt_tf2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","f:\\1-workdir\\pyt_tf2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","f:\\1-workdir\\pyt_tf2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","        a0-b       0.60      0.28      0.38     19850\n","        a0-i       0.53      0.24      0.33     37848\n","         p-b       0.94      0.96      0.95     21028\n","        a1-b       0.43      0.21      0.29     17042\n","        a1-i       0.49      0.34      0.40     49657\n","           o       0.84      0.97      0.90    563306\n","        a2-b       0.00      0.00      0.00      6135\n","        a2-i       0.46      0.01      0.02     17475\n","         p-i       0.65      0.14      0.24      1475\n","        a3-b       0.00      0.00      0.00      1256\n","        a3-i       0.00      0.00      0.00      3545\n","        a5-b       0.00      0.00      0.00        20\n","        a4-b       0.00      0.00      0.00       189\n","        a4-i       0.00      0.00      0.00       598\n","        a5-i       0.00      0.00      0.00        32\n","\n","    accuracy                           0.81    739456\n","   macro avg       0.33      0.21      0.23    739456\n","weighted avg       0.77      0.81      0.78    739456\n","\n","Evaluating val\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"38f404831cbc48148d4e2790f3b507cb","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/34 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","        a0-b       0.61      0.28      0.38      1569\n","        a0-i       0.56      0.19      0.28      3918\n","         p-b       0.90      0.94      0.92      1566\n","        a1-b       0.53      0.22      0.31      1424\n","        a1-i       0.57      0.21      0.31      6822\n","           o       0.72      0.98      0.83     34546\n","        a2-b       0.00      0.00      0.00       620\n","        a2-i       0.20      0.00      0.00      2151\n","         p-i       0.61      0.21      0.31       110\n","        a3-b       0.00      0.00      0.00       173\n","        a3-i       0.00      0.00      0.00       538\n","        a5-b       0.00      0.00      0.00         1\n","        a4-b       0.00      0.00      0.00        25\n","        a4-i       0.00      0.00      0.00        65\n","        a5-i       0.00      0.00      0.00         8\n","\n","    accuracy                           0.71     53536\n","   macro avg       0.31      0.20      0.22     53536\n","weighted avg       0.65      0.71      0.64     53536\n","\n","Evaluating test\n"]},{"name":"stderr","output_type":"stream","text":["f:\\1-workdir\\pyt_tf2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","f:\\1-workdir\\pyt_tf2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","f:\\1-workdir\\pyt_tf2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fa6a301c28d54f6b87924918836b3b7b","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/35 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","        a0-b       0.61      0.25      0.36      1637\n","        a0-i       0.52      0.17      0.26      3910\n","         p-b       0.89      0.93      0.91      1612\n","        a1-b       0.50      0.18      0.26      1485\n","        a1-i       0.59      0.21      0.31      6575\n","           o       0.72      0.98      0.83     36174\n","        a2-b       0.00      0.00      0.00       654\n","        a2-i       0.38      0.00      0.01      2460\n","         p-i       0.48      0.11      0.17       104\n","        a3-b       0.00      0.00      0.00       153\n","        a3-i       0.00      0.00      0.00       496\n","        a5-b       0.00      0.00      0.00         1\n","        a4-b       0.00      0.00      0.00        25\n","        a4-i       0.00      0.00      0.00        72\n","        a5-i       0.00      0.00      0.00         2\n","\n","    accuracy                           0.71     55360\n","   macro avg       0.31      0.19      0.21     55360\n","weighted avg       0.65      0.71      0.64     55360\n","\n"]},{"name":"stderr","output_type":"stream","text":["f:\\1-workdir\\pyt_tf2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","f:\\1-workdir\\pyt_tf2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","f:\\1-workdir\\pyt_tf2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}],"source":["# A8:SanityCheck\n","\n","for fold, dx in [('train', train), ('val', val), ('test', test)]:\n","    print(f'Evaluating {fold}')\n","    print(eval_tagger(best_bilstm, dx, label_names=[ner_vocab.decode(x) for x in range(15)]))"]},{"cell_type":"markdown","metadata":{"id":"y3W76wMl_qSH"},"source":["### 9. (8 pts) Inference: How does the tagger perform?\n","\n","In this final question, you will complete the `inference` function, which will attempt to apply the model to new data. \n","\n","While we've setup the POS tagging of the raw text, you should:\n","* Grab the predicate and its POS\n","* Properly form the input (make input into a batch size of 1 with `.unsqueeze`)\n","* Obtain raw predictions\n","* Use `torch.argmax` to obtain the most likely classes and store them in the variable `preds`\n","\n","After this implementation, you'll notice that we do a number of additional post-processing steps. Quoting from the [original paper](https://www.aclweb.org/anthology/N18-1081.pdf):\n","\n","> \"At inference time, we first identify all verbs and nominal predicates in the sentence as candidate predicate heads. We use a Part Of Speech (POS) tagger to identify verbs, and Catvar’s subcategorization frames (Habash and Dorr, 2003) for nominalizations, identifying nouns which share the same frame with a verbal equivalent (e.g., acquisition with acquire). We then generate an input instance for each candidate predicate head. For each instance, we tag each word with its most likely BIO label under the model, and reconstruct Open IE tuples from the resulting sequence according to the method described in Section 3, with the exception that we ignore malformed spans (i.e., if an A0-I label is not preceded by A0-I or A0-B, we treat it as O).\"\n","\n","While, perhaps, not quite as involved, you'll notice that we replicate several of these steps below, with the additional grounding that, if we're supplied the start of the predicate, we should probably use that!"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to C:\\Users\\Tai\n","[nltk_data]     Nguyen\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}],"source":["import nltk\n","nltk.download('punkt')\n","from nltk import pos_tag, word_tokenize"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"usixL4rV_-bu"},"outputs":[],"source":["# A9:Function(8/8)\n","# import nltk\n","# nltk.download('punkt')\n","# from nltk import pos_tag, word_tokenize\n","\n","def inference(model, text, pred_idx, token_vocab, pos_vocab, ner_vocab, max_length=32):\n","    \n","    # Grab our tokens\n","    tokens = word_tokenize(text.lower())\n","    \n","    # Grab our POS tags\n","    pos_tags = [t[1].lower() for t in pos_tag(tokens)]\n","\n","    # --- Your code starts here ---\n","    model = model.eval()\n","    len_toks = len(tokens)\n","    npads = max_length - len_toks\n","    tokens = (tokens + [''] * (npads))[:max_length]\n","    pos_tags = (pos_tags + [''] * (npads))[:max_length]\n","    pos_tags = list(map(lambda s: s.lower(), pos_tags))\n","\n","    p_tok = ([ tokens[pred_idx] ] * len_toks + [''] * (npads))[:max_length]\n","    p_pos = ([ pos_tags[pred_idx].lower() ] * len_toks + [''] * (npads))[:max_length]\n","\n","    enc_tokens = torch.LongTensor(list(map(token_vocab.encode, tokens))).unsqueeze(0).cuda()\n","    enc_pos_tags = torch.LongTensor(list(map(pos_vocab.encode, pos_tags))).unsqueeze(0).cuda()\n","    enc_p_tok = torch.LongTensor(list(map(token_vocab.encode, p_tok))).unsqueeze(0).cuda()\n","    enc_p_pos = torch.LongTensor(list(map(pos_vocab.encode, p_pos))).unsqueeze(0).cuda()\n","\n","    with torch.no_grad():\n","        preds = model(enc_tokens, enc_pos_tags, enc_p_tok, enc_p_pos)\n","        preds = preds.squeeze()\n","        preds = torch.argmax(preds, dim=1)\n","    \n","    # --- Your code ends here --- \n","    \n","    preds = preds.squeeze()\n","    preds = preds.detach().cpu().tolist()\n","\n","    # Transform predictions into their string labels\n","    preds = [ner_vocab.decode(p) for p in preds]\n","\n","    # Did we actually predict the predicate as the predicate?\n","    if preds[pred_idx] != 'p-b':\n","        preds[pred_idx] = 'p-b'\n","\n","    # Filter completely un-reasonable predictions (aX-i when not proceeded by aX-b/i)\n","    for ix, p in enumerate(preds):\n","        if '-i' in p:\n","            # Gets the core of the prediction;\n","            # This will be p, a0, a1, ...\n","            core = p.split('-')[0]\n","            if not ix:\n","                # The first prediction cannot be an -i tag\n","                preds[ix] = 'o'\n","            elif core not in preds[ix - 1]:\n","                # The previous tag does not match, so sequence in incompatible\n","                preds[ix] = 'o'\n","\n","    # Returning two formats for our inspection!\n","    zip_out = list(zip(tokens, preds))\n","    tuple_out = []\n","    prev = ''\n","    for ix, (t, p) in enumerate(zip_out):\n","        # Properly segmenting portions of the tuple form with ';\n","        if ix and p != prev and tuple_out[-1] != ';':\n","            tuple_out.append(';')\n","        \n","        if p != 'o':\n","            tuple_out.append(t)\n","        \n","        # Update previous for segmenting with ';'\n","        prev = p\n","    tuple_out = tuple(tuple_out)\n","\n","    return zip_out, tuple_out\n"]},{"cell_type":"markdown","metadata":{"id":"X0FJlCNfewkp"},"source":["As a sanity check, you should anticipate something like so:\n","\n","```\n","Test 0\n","([('the', 'a0-b'), ('assignment', 'a0-b'), ('was', 'p-b'), ('the', 'a4-b'), ('last', 'o'), ('one', 'o')], ('the', 'assignment', ';', 'was', ';', 'the', ';'))\n","\n","Test 1\n","([('the', 'a0-b'), ('students', 'a0-b'), ('completed', 'p-b'), ('the', 'a4-b'), ('assignment', 'a5-b')], ('the', 'students', ';', 'completed', ';', 'the', ';', 'assignment'))\n","```\n","\n","which, while not perfect, is actually not too shabby!\n","\n","Additionally, we've built out a smaller model than the exact one in the original paper with word vectors of 50 dimensions (the original paper uses 300 dimensional vectors). However, we've gotten within 10 points of the previous model and all without any hyperparameter tuning nor fancy additions like attention!"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"HeU-Vo8aWU94"},"outputs":[{"name":"stdout","output_type":"stream","text":["Test 0\n","([('the', 'a0-b'), ('assignment', 'a0-i'), ('was', 'p-b'), ('the', 'a1-b'), ('last', 'a1-i'), ('one', 'o'), ('', 'o'), ('', 'o'), ('', 'o'), ('', 'o'), ('', 'o'), ('', 'o'), ('', 'o'), ('', 'o'), ('', 'o'), ('', 'o'), ('', 'o'), ('', 'o'), ('', 'o'), ('', 'o'), ('', 'o'), ('', 'o'), ('', 'o'), ('', 'o'), ('', 'o'), ('', 'o'), ('', 'o'), ('', 'o'), ('', 'o'), ('', 'o'), ('', 'o'), ('', 'o')], ('the', ';', 'assignment', ';', 'was', ';', 'the', ';', 'last', ';'))\n","\n","Test 1\n","([('the', 'a0-b'), ('students', 'a0-i'), ('completed', 'p-b'), ('the', 'a1-b'), ('assignment', 'o'), ('', 'o'), ('', 'o'), ('', 'o'), ('', 'o'), ('', 'o'), ('', 'o'), ('', 'o'), ('', 'o'), ('', 'o'), ('', 'o'), ('', 'o'), ('', 'o'), ('', 'o'), ('', 'o'), ('', 'o'), ('', 'o'), ('', 'o'), ('', 'o'), ('', 'o'), ('', 'o'), ('', 'o'), ('', 'o'), ('', 'o'), ('', 'o'), ('', 'o'), ('', 'o'), ('', 'o')], ('the', ';', 'students', ';', 'completed', ';', 'the', ';'))\n","\n"]}],"source":["# A9:SanityCheck\n","\n","tests = [\n","         ('The assignment was the last one', 2),\n","         ('The students completed the assignment', 2)\n","]\n","\n","for ix, (test_str, pred_idx) in enumerate(tests):\n","  print('Test', ix)\n","  result = inference(best_bilstm, test_str, pred_idx, token_vocab, pos_vocab, ner_vocab)\n","  print(result)\n","  print()"]},{"cell_type":"markdown","metadata":{"id":"7UKbKy5Peejq"},"source":["As a final remark, notice that in this final inferential step, we applied our knowledge about the grammar of the task to refine the potential prediction our network would produce. For models and tasks like this, especially with limited data (~10,000 samples versus ~1,000,000+ samples for \"big\" datasets), it's important to observe that we gain by informing ourselves about the expected grammar of the task. \n","\n","Furthermore, if you review the original paper, you'll notice that they further improve their model by developing a notion of \"confidence\" in prediction and using that to further filter results. \n","Additionally, there are ways like [Conditional Random Fields (CRFs)](https://homepages.inf.ed.ac.uk/csutton/publications/crftut-fnt.pdf) that may be used to enforce constraints like grammars on final outputs. \n","This is all beyond the scope of this current assignment, however, they're worth mentioning because they can be vital tricks that takes a model like we've built here from one that works kinda-alright into a model that works decently well in a practical application."]},{"cell_type":"markdown","metadata":{"id":"PYEDeqLrAQfV"},"source":["### Beyond this Assignment: Hyperparameter Tuning\n","\n","One thing that we should make note of is the fact that our model has a lot of hyperparameters. These are our set of parameters that aren't adjusted during training, rather they set the structure for how training is allowed to proceed.\n","In all likelihood, _much better_ hyperparameter selections exist.\n","\n","Architecturally, our model has hyperparameters like the number of layers, the hidden dimension, the word and POS embedding dimensions, the dropout rates, and the usage of bidirectionality (or not).\n","\n","When it comes to learning, there are another segment of hyperparameters that we introduce like the learning rate and the batch size. In a way, even our selection of optimizer introduces another hyperparameter. \n","\n","Finally, consider how our pre-processing of our data also has hyperparameters associated with it: pad length and lowercasing. Though the latter of the two is necessary to be compatible with the set of pre-trained vectors we've used, it is good to acknowledge that this kind of pre-processing, inevitably, will affect downstream performance (as we've seen this term!).\n","\n","How does one decide which hyperparameters are best? What are the approaches used to try different combinations of hyperparameters? \n","\n","A typical flow will have a model train on the train set, while the quality of the hyperparameters should be evaluated relative to the validation set (with the final test set as our true test that we should __never__ touch until final evaluation). Some of the hyperparameter tuning approaches you may come across:\n","* __Grid Search__: For each hyperparameter, enumerate any values you consider to be appropriate. Then, train a model for every combination of parameters. Notice that the \"every combination\" part can result in a very large set of combinations to consider. \n","* __Random Walk__: Fix a budget for the number of models you are willing to consider and train that many models. For each new model, randomly generate the hyperparameters (within a specified set of options or range for each hyperparamter).\n","* __Bayesian Optimization__: Use the past experiments to inform the selection of hyperparameters for new experiments. Typically, this type of approach will have an engine that generates the parameters for you based on information you supply about a model's performance. \n","\n","There are many packages and applications that facilitate hyperparameter tuning. One such application that is extremely useful for training and debugging deep learning models is [Weights & Biases](https://wandb.ai/site), also known as `wandb`.\n","\n","wandb is excellent for monitoring experiments, visualizing loss curves, tuning hyperparameters, and more! Once creating an account with wandb, it's merely a matter of installing its Python package and using basic commands like `wb.init(...)` to specify information about your experiment and `wb.log(...)` to push performance information to wandb. For more information, check out [this guide](https://docs.wandb.ai/quickstart) for rapidly getting started with wandb.\n","\n","You are __highly encouraged__ to make use of such packages, either in your final project or in deep learning projects you take on beyond this course. \n","There's nothing required for this portion, just merely a reflection on best practices and where one would be recommended to proceed beyond this point!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b_e8hraWGwTs"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"A5-Module-A-v2.ipynb","provenance":[],"toc_visible":true},"interpreter":{"hash":"36ae27ce247b11bfccb9cabcb4b15cfcf0c4bab2062d9ce38edefc649ee031a4"},"kernelspec":{"display_name":"Python 3.9.13 ('pyt_tf2')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":0}
